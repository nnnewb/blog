<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>kubernetes on weakptr's 笔记</title><link>https://nnnewb.github.io/categories/kubernetes/</link><description>Recent content in kubernetes on weakptr's 笔记</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><lastBuildDate>Mon, 27 Dec 2021 15:21:00 +0800</lastBuildDate><atom:link href="https://nnnewb.github.io/categories/kubernetes/index.xml" rel="self" type="application/rss+xml"/><item><title>排查一个kubectl无反应的问题</title><link>https://nnnewb.github.io/p/why-my-kubectl-not-responding/</link><pubDate>Mon, 27 Dec 2021 15:21:00 +0800</pubDate><guid>https://nnnewb.github.io/p/why-my-kubectl-not-responding/</guid><description>&lt;p>懒得分段了，就当做是讲个故事吧。&lt;/p>
&lt;p>背景大概是这样。&lt;/p>
&lt;p>内网公共开发机上配置了 k3s 集群，同时后端开发工作也在这台开发机上进行（通过vscode remote-ssh）。因为公司太抠门，开发机只有117G硬盘容量，除去必要的开发工具、系统环境之类的东西，实际可用一直没超过50%，机器上又跑了很多东西，像是 gitlab-runner、docker的registry、MySQL、elasticsearch、开发集群服务等等，差不多每一两个星期都会出现 disk-pressure 的 taint，导致 pod 被 evicted。实话说能跑就很满足了，毕竟公司抠门到开发部门的上行带宽都贼小，如果把镜像推送到公网的registry去部署的话体验更差。&lt;/p>
&lt;p>今天（周一）来公司之后调了下gitlab-ci，给一个前端项目做持续部署。因为前端对kubernetes这套不熟悉，也没有相关的服务器权限，总之就是很难让他们自己来。但是产品部门又喜欢提那种“按钮移到右上角”、“加个图片”之类的需求（对，我司还没有需求管理系统，开发就是个撸码的无情工具人），前端老是过来找我去部署下环境，就搞得摸鱼都摸不痛快。&lt;/p>
&lt;p>所以，当当当~当~，整一个持续部署呗，反正是个纯前端项目，不用部署配套的后端代码，写个dockerfile再写个helm chart就差不多了，ci调了调构建镜像就完事，不过因为ci部署需要访问集群，所以又改了下&lt;code>.kube/config&lt;/code>，删了之前尝试&lt;code>csr&lt;/code>方式添加用户的时候加多的 user 和 context ，复制了一份挂载到 runner 容器里。&lt;/p>
&lt;p>然后&amp;hellip;&amp;hellip;问题就来了。&lt;/p>
&lt;p>同事忽然告诉我办公室的服务挂了，于是下意识地打出&lt;code>kgp&lt;/code>，卡住。&lt;/p>
&lt;p>等了一会儿，还是卡住。&lt;/p>
&lt;p>又等了一会儿，坐不住了。试了下&lt;code>kubectl cluster-info&lt;/code>，继续卡住。&lt;/p>
&lt;p>开始慌了，想起今天的机器有点卡，先看看 &lt;code>free -h&lt;/code> 有没有内存泄漏之类的问题导致阻塞，结果发现并没有，于是继续看 &lt;code>htop&lt;/code>，cpu使用率也比较正常。再看&lt;code>df -h | grep -vE 'shm|overlay'&lt;/code>，发现硬盘使用率96%（估计硬盘主控想死的心都有了，揪着4%的可用空间想把PE数平均到各个区块恐怕不容易）。&lt;/p>
&lt;p>找到问题后松了口气，十有八九是又出现 evicted 了。二话不说直接 &lt;code>docker system df&lt;/code>，看到30多G的 build cache 顿时惊了，肯定不是go的构建缓存（手动挂载优化了），那就是 node_modules 又立奇功了。node_modules=黑洞果然不是吹的。&lt;/p>
&lt;p>清理完使用率恢复到63%，但依然有种不安感萦绕于心，于是再次尝试&lt;code>kgp&lt;/code>，卡住。&lt;/p>
&lt;p>等了一会儿，喝口水，继续卡着。&lt;/p>
&lt;p>又等了一会儿，淦。&lt;/p>
&lt;p>想了想，&lt;code>journalctl -r -u k3s&lt;/code>看看日志，并没有什么发现，倒是注意到很多&lt;code>linkerd&lt;/code>之类的我们部门经理搞事的时候遗留下来的玩意儿在报错，service mesh 我不熟，但寻思应该不会影响 kubectl 吧，k3s 本体的 api-server 应该不归 linkerd 管。更何况 linkerd 本身就没配好。再翻了翻看到下面的内容。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;span class="lnt">8
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback"> 6 12月 25 21:16:07 office k3s[794]: I1225 13:16:07.685149 794 container_gc.go:85] attempting to delete unused containers
7 12月 25 21:16:07 office k3s[794]: I1225 13:16:07.687723 794 image_gc_manager.go:321] attempting to delete unused images
8 12月 25 21:16:07 office k3s[794]: I1225 13:16:07.782390 794 eviction_manager.go:351] eviction manager: able to reduce ephemeral-storage pressure without evicting pods.
9 12月 25 21:16:17 office k3s[794]: W1225 13:16:17.939242 794 eviction_manager.go:344] eviction manager: attempting to reclaim ephemeral-storage
10 12月 25 21:16:17 office k3s[794]: I1225 13:16:17.939267 794 container_gc.go:85] attempting to delete unused containers
11 12月 25 21:16:17 office k3s[794]: I1225 13:16:17.941771 794 image_gc_manager.go:321] attempting to delete unused images
12 12月 25 21:16:18 office k3s[794]: I1225 13:16:18.033724 794 eviction_manager.go:351] eviction manager: able to reduce ephemeral-storage pressure without evicting pods.
13 12月 25 21:16:28 office k3s[794]: W1225 13:16:28.214032 794 eviction_manager.go:344] eviction manager: attempting to reclaim ephemeral-storage
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>这个是老问题了，一直没去研究怎么解决。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback"> 154 12月 25 21:21:55 office k3s[794]: I1225 13:21:55.021937 794 image_gc_manager.go:304] [imageGCManager]: Disk usage on image filesystem is at 95% which is over the high threshold (85%). Trying to free 182 155 12月 25 21:21:55 office k3s[794]: E1225 13:21:55.025140 794 kubelet.go:1292] Image garbage collection failed multiple times in a row: failed to garbage collect required amount of images. Wanted to free
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>这次搜了下，应该是 &lt;code>docker system prune&lt;/code> 造成 &lt;code>kubelet&lt;/code> 找不到可回收的镜像才报错（猜测），不过依然不能解释为啥 &lt;code>kubectl&lt;/code> 没反应。于是继续翻了会儿日志，搜索错误，但始终没有什么结果。&lt;/p>
&lt;p>但是同事还要干活，没辙了，先重启下服务器吧。群里说了一声要重启了，等了一会儿跑&lt;code>sudo reboot&lt;/code>，重启完连接，继续&lt;code>kgp&lt;/code>，卡住。&lt;/p>
&lt;p>嗯&amp;hellip;&amp;hellip;&lt;/p>
&lt;p>早有预料。&lt;/p>
&lt;p>&lt;code>journalctl -r -u k3s --boot&lt;/code> 看看重启后的日志，发现还是老一套的问题，&lt;code>docker&lt;/code> 手动处理镜像和容器造成的和 kubernetes 的管理机制的冲突，各种找不到镜像或者容器的警告，还有一些错误和trace，但没有一个能解释为什么&lt;code>kubectl&lt;/code>没有反应。。。&lt;/p>
&lt;p>直到在&lt;code>kubectl&lt;/code>的终端里按下了ctrl+c，在顺手&lt;code>clear&lt;/code>之前看到了一行请输入用户名（eng）&amp;hellip;&lt;/p>
&lt;p>警觉。&lt;/p>
&lt;p>忽然想起来，因为 &lt;code>kubectl&lt;/code> 这破玩意儿是没有颜色输出的，用习惯了各种彩色输出的命令行工具，&lt;code>kubectl&lt;/code>就格外不顺眼。所以在发现&lt;code>kubecolors&lt;/code>后，我就直接把&lt;code>kubectl&lt;/code>配置成了&lt;code>kubecolor&lt;/code>的别名。&lt;/p>
&lt;p>所以&amp;hellip;&amp;hellip;难道是&lt;code>kubecolor&lt;/code>的问题？&lt;/p>
&lt;p>&lt;code>whereis kubectl&lt;/code>找到&lt;code>kubectl&lt;/code>的绝对路径之后，尝试手动运行&lt;code>/usr/local/bin/kubectl cluster-info&lt;/code>，再次出现了那个输入用户名的提示，顿时开始怀疑起&lt;code>.kube/config&lt;/code>配置有问题，正好在出现问题之前改过了&lt;code>.kube/config&lt;/code>，这里出问题的嫌疑就很吉尔大。&lt;/p>
&lt;p>于是打开&lt;code>.kube/config&lt;/code>，检查了一下集群的用户配置，发现果然，是我手欠把办公室集群的用户给删了。草。&lt;/p>
&lt;p>急忙从&lt;code>/etc/rancher/k3s/k3s.yaml&lt;/code>复制下用户证书配置，贴进去，再运行&lt;code>kgp&lt;/code>果然屁事没有了。&lt;/p>
&lt;p>所以总结如下。&lt;/p>
&lt;ol>
&lt;li>别被表面的问题迷惑。&lt;/li>
&lt;li>自己犯傻的几率大于基础设施/常用工具犯傻的几率。&lt;/li>
&lt;li>遇到问题解决步骤很重要，准确的方向可以省很多时间。
&lt;ol>
&lt;li>先确定故障表现和复现条件&lt;/li>
&lt;li>确定故障点（出现在网络、网关还是应用、数据库），弄清楚是不是新问题&lt;/li>
&lt;li>再排查相关配置是否正确，回忆是否有做过相关修改变更&lt;/li>
&lt;li>再排查故障点日志，必要的时候参考下代码，毕竟有的时候日志没写清楚错误的上下文&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ol></description></item><item><title>简单的ECK部署</title><link>https://nnnewb.github.io/p/simple-eck-cluster-deployment/</link><pubDate>Tue, 30 Nov 2021 11:13:00 +0800</pubDate><guid>https://nnnewb.github.io/p/simple-eck-cluster-deployment/</guid><description>&lt;h2 id="前言">前言&lt;/h2>
&lt;p>因为工作需要，得在自己搭建的集群里部署一个 Elasticsearch 。又因为是云端的集群，在 k8s 外用 docker 单独起一个 ES 明显更难维护（但部署更简单），于是选择用 ECK 。&lt;/p>
&lt;p>ECK 就是 Elastic Cloud on Kubernetes 的缩写，可以理解成部署在 Kubernetes 上的 Elasticsearch 。当然不止 ES 。&lt;/p>
&lt;p>部署 ES 的过程遇到几个问题记录下怎么解决的。&lt;/p>
&lt;ol>
&lt;li>ES 使用自签名证书，导致 HTTP 不能连接。&lt;/li>
&lt;li>ECK 需要安装 IK 分词插件。&lt;/li>
&lt;li>ECK 默认密码每次部署都重新生成，而且默认用户权限过大。&lt;/li>
&lt;li>ECK 默认没配 PVC ，数据没有持久化。&lt;/li>
&lt;/ol>
&lt;p>接下来逐个解决。&lt;/p>
&lt;h2 id="0x01-自签名证书">0x01 自签名证书&lt;/h2>
&lt;p>自签名证书解决方法有几个&lt;/p>
&lt;ol>
&lt;li>改客户端，让客户端用自签名证书连接。很麻烦。&lt;/li>
&lt;li>生成一个固定的证书，让ES和客户端都用这个证书，客户端和ES都要改。很麻烦。&lt;/li>
&lt;li>禁用 ES 的自签名证书。&lt;/li>
&lt;/ol>
&lt;p>考虑到是私有的测试环境，不搞这些烦人的东西，直接禁用。&lt;/p>
&lt;p>修改 YAML 如下。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;span class="lnt">8
&lt;/span>&lt;span class="lnt">9
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-yaml" data-lang="yaml">&lt;span class="nt">apiVersion&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">elasticsearch.k8s.elastic.co/v1&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w">&lt;/span>&lt;span class="nt">kind&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">Elasticsearch&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w">&lt;/span>&lt;span class="nt">metadata&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">elasticsearch&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w">&lt;/span>&lt;span class="nt">spec&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">http&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">tls&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">selfSignedCertificate&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">disabled&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kc">true&lt;/span>&lt;span class="w">
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>注意 &lt;code>spec.http.tls.selfSignedCertificate.disabled&lt;/code> 这个字段。&lt;/p>
&lt;p>参考文档：&lt;a class="link" href="https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-tls-certificates.html" target="_blank" rel="noopener"
>Orchestrating Elastic Stack applications - Access Elastic Stack services - TLS certificates&lt;/a>&lt;/p>
&lt;h2 id="0x02-安装-ik-分词组件">0x02 安装 IK 分词组件&lt;/h2>
&lt;p>官方文档提供的安装插件思路是利用 initContainer 。参考文档：&lt;a class="link" href="https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-init-containers-plugin-downloads.html" target="_blank" rel="noopener"
>init containers for plugin downloads&lt;/a> 。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-yaml" data-lang="yaml">&lt;span class="nt">spec&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">nodeSets&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>- &lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">default&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">count&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="m">3&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">podTemplate&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">spec&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">initContainers&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>- &lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">install-plugins&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">command&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>- &lt;span class="l">sh&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>- -&lt;span class="l">c&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>- &lt;span class="p">|&lt;/span>&lt;span class="sd">
&lt;/span>&lt;span class="sd"> &lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">bin/elasticsearch-plugin install --batch repository-gcs&lt;/span>&lt;span class="w">
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>initContainer 容器默认会继承自下面的内容：&lt;/p>
&lt;ul>
&lt;li>没有另外指定的情况下，继承主容器的镜像(我的例子中，就是 &lt;code>Elasticsearch:7.9.1&lt;/code>)&lt;/li>
&lt;li>主容器的 volume 挂载，如果 initContainer 有同名同路径的 volume 则优先用 initContainer 的。&lt;/li>
&lt;li>POD 名称和 IP 。&lt;/li>
&lt;/ul>
&lt;h2 id="0x03-添加自定义用户">0x03 添加自定义用户&lt;/h2>
&lt;p>有好几种方式：&lt;/p>
&lt;ol>
&lt;li>官方文档中的方法：&lt;a class="link" href="https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-users-and-roles.html" target="_blank" rel="noopener"
>k8s users and roles&lt;/a>，比较稳定，但还是挺麻烦的。&lt;/li>
&lt;li>修改 &lt;code>[es-cluster-name]-es-elastic-user&lt;/code> 这个 &lt;code>secret&lt;/code>，好处是简单，但要求必须先创建 secret 再创建 ES ，单个 YAML 去 &lt;code>create -f&lt;/code> 的情况下不友好。&lt;/li>
&lt;li>基于第2节中利用 initContainer 的做法和官方文档里提到的 &lt;code>elasticsearch-users&lt;/code> 命令行工具，直接在 initContainer 里创建指定用户名密码的用户。不确定这个做法会不会在多节点 ECK 里出问题，毕竟这等于是每个节点都创建了一次用户。不过我只需要单节点，所以也还过得去。&lt;/li>
&lt;/ol>
&lt;p>最终决定用第 3 种方法，因为做一个单节点集群简单不费事，多节点的话，目前开的服务器配置也吃不消。（其实是搞完才仔细读文档，第 1 种方法其实也不算太麻烦&amp;hellip;）&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-yaml" data-lang="yaml">&lt;span class="nt">spec&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">nodeSets&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>- &lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">default&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">count&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="m">3&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">podTemplate&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">spec&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">initContainers&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>- &lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">donviewclass-initialize&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">command&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>- &lt;span class="l">sh&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>- -&lt;span class="l">c&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>- &lt;span class="p">|&lt;/span>&lt;span class="sd">
&lt;/span>&lt;span class="sd"> ./bin/elasticsearch-plugin install -batch https://ghproxy.com/https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v7.9.1/elasticsearch-analysis-ik-7.9.1.zip
&lt;/span>&lt;span class="sd"> ./bin/elasticsearch-users useradd tsdonviewclass -p tsdonviewclass -r superuser&lt;/span>&lt;span class="w">
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;code>./bin/elasticsearch-users useradd tsdonviewclass -p tsdonviewclass -r superuser&lt;/code> 主要就是增加这一句。同样是因为懒，权限直接给了 superuser 。&lt;/p>
&lt;p>参考文档：&lt;a class="link" href="https://www.elastic.co/guide/en/elasticsearch/reference/current/users-command.html" target="_blank" rel="noopener"
>elasticsearch-users&lt;/a> 。&lt;/p>
&lt;h2 id="0x04-配置pvc">0x04 配置PVC&lt;/h2>
&lt;p>依然是参考官方文档来：&lt;a class="link" href="https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-volume-claim-templates.html" target="_blank" rel="noopener"
>k8s-volume-claim-templates&lt;/a>。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-yaml" data-lang="yaml">&lt;span class="nt">spec&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">nodeSets&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>- &lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">default&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">count&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="m">1&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">config&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">node.store.allow_mmap&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kc">false&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">volumeClaimTemplates&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>- &lt;span class="nt">metadata&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">elasticsearch-data&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="c"># Do not change this name unless you set up a volume mount for the data path.&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">spec&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">accessModes&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>- &lt;span class="l">ReadWriteOnce&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">resources&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">requests&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">storage&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">5Gi&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">storageClassName&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">local-path&lt;/span>&lt;span class="w">
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>注意 &lt;code>volumeClaimTemplates&lt;/code> 下 &lt;code>metadata.name&lt;/code> 不要变，除非你自己在 &lt;code>podTemplate&lt;/code> 里覆写挂载字段。&lt;/p>
&lt;p>其他的 &lt;code>spec&lt;/code> 下内容和通常的 PVC 一样，可以参考 &lt;a class="link" href="https://kubernetes.io/zh/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims" target="_blank" rel="noopener"
>Kubernetes - PersistentVolumeClaims&lt;/a> 。&lt;/p>
&lt;p>值得注意的是 ECK 默认在集群节点数量 scaled down 时删除 PVC ，对应的 PV 可能保留，具体看&lt;a class="link" href="https://kubernetes.io/docs/concepts/storage/storage-classes/#reclaim-policy" target="_blank" rel="noopener"
>存储类的回收策略&lt;/a>。ECK 的 CRD 里也给了相关的配置项。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-yaml" data-lang="yaml">&lt;span class="nt">apiVersion&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">elasticsearch.k8s.elastic.co/v1&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w">&lt;/span>&lt;span class="nt">kind&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">Elasticsearch&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w">&lt;/span>&lt;span class="nt">metadata&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">es&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w">&lt;/span>&lt;span class="nt">spec&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">version&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="m">7.15.2&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">volumeClaimDeletePolicy&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">DeleteOnScaledownOnly&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">nodeSets&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>- &lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">default&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">count&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="m">3&lt;/span>&lt;span class="w">
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>注意 &lt;code>volumeClaimDeletePolicy: DeleteOnScaledownOnly&lt;/code> 。可选的策略包括：&lt;/p>
&lt;ul>
&lt;li>&lt;code>DeleteOnScaledownAndClusterDeletion&lt;/code>&lt;/li>
&lt;li>&lt;code>DeleteOnScaledownOnly&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>默认策略是 &lt;code>DeleteOnScaledownAndClusterDeletion&lt;/code> ，集群删除和 scaled down 时删除 PVC。&lt;/p>
&lt;p>如果是一次性的部署，可以直接用 &lt;code>emptyDir&lt;/code> 作为存储类，不用管数据丢不丢。&lt;/p>
&lt;h2 id="总结">总结&lt;/h2>
&lt;p>这几步配置下来，一个开发用的 ES 集群就算是配完了，资源给够就能开始玩了。&lt;/p>
&lt;p>讲道理我不太会运维 ES 啊，ES 这东西实在有点重量级，现阶段的能力也就只能看文档这里那里配一下，在上面开发什么的。真要遇到大问题还得抓瞎。&lt;/p>
&lt;p>就先这样吧。&lt;/p></description></item><item><title>kubeadm安装实验集群记录</title><link>https://nnnewb.github.io/p/kubernetes-manually-install-by-kubeadm/</link><pubDate>Thu, 25 Nov 2021 14:31:00 +0800</pubDate><guid>https://nnnewb.github.io/p/kubernetes-manually-install-by-kubeadm/</guid><description>&lt;h2 id="前言">前言&lt;/h2>
&lt;p>好吧，如果仔细想想就会发现不管是 k3s 还是 ucloud 上的 k8s ，都没有一个是自己手动配置好的。虽说并不是至关重要的，但手动用 kubeadm 装一次 kubernetes 总不会有什么坏处。顺手做个笔记。参考资料列出如下。&lt;/p>
&lt;ul>
&lt;li>&lt;a class="link" href="https://kubernetes.io/zh/docs/setup/production-environment/tools/kubeadm/install-kubeadm/" target="_blank" rel="noopener"
>https://kubernetes.io/zh/docs/setup/production-environment/tools/kubeadm/install-kubeadm/&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://mirrors.tuna.tsinghua.edu.cn/help/kubernetes/" target="_blank" rel="noopener"
>https://mirrors.tuna.tsinghua.edu.cn/help/kubernetes/&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://computingforgeeks.com/deploy-kubernetes-cluster-on-ubuntu-with-kubeadm/" target="_blank" rel="noopener"
>https://computingforgeeks.com/deploy-kubernetes-cluster-on-ubuntu-with-kubeadm/&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="系统配置">系统配置&lt;/h2>
&lt;p>正式安装之前先确认一些系统级配置。&lt;/p>
&lt;h3 id="swapoff">swapoff&lt;/h3>
&lt;p>简单的做法是 &lt;code>sudo swapoff -a&lt;/code> 即可。之后改 &lt;code>fstab&lt;/code> 把 &lt;code>swap&lt;/code> 分区关掉。&lt;/p>
&lt;h3 id="iptables检查桥接流量">iptables检查桥接流量&lt;/h3>
&lt;p>用 &lt;code>lsmod | grep bf_netfitler&lt;/code> 检查有没有启用 &lt;code>bf_netfilter&lt;/code> 模块，如果没有输出的话说明没加载，执行下面的命令。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">cat &lt;span class="s">&amp;lt;&amp;lt;EOF | sudo tee /etc/modules-load.d/k8s.conf
&lt;/span>&lt;span class="s">br_netfilter
&lt;/span>&lt;span class="s">EOF&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>会在 &lt;code>/etc/modules-load.d&lt;/code> 下添加一个模块自动加载的配置。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">cat &lt;span class="s">&amp;lt;&amp;lt;EOF | sudo tee /etc/sysctl.d/k8s.conf
&lt;/span>&lt;span class="s">net.bridge.bridge-nf-call-ip6tables = 1
&lt;/span>&lt;span class="s">net.bridge.bridge-nf-call-iptables = 1
&lt;/span>&lt;span class="s">EOF&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>再在 &lt;code>/etc/sysctl.d/&lt;/code> 下添加一个配置，允许 &lt;code>iptables&lt;/code> 查看桥接流量。&lt;/p>
&lt;p>然后用 &lt;code>sysctl&lt;/code> 重载配置。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">sudo sysctl --system
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="端口">端口&lt;/h3>
&lt;p>控制平面节点的端口清单，如果有本机防火墙的话需要开放下面的端口。&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>协议&lt;/th>
&lt;th>方向&lt;/th>
&lt;th>端口范围&lt;/th>
&lt;th>作用&lt;/th>
&lt;th>使用者&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>TCP&lt;/td>
&lt;td>入站&lt;/td>
&lt;td>6443&lt;/td>
&lt;td>Kubernetes API 服务器&lt;/td>
&lt;td>所有组件&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>TCP&lt;/td>
&lt;td>入站&lt;/td>
&lt;td>2379-2380&lt;/td>
&lt;td>etcd 服务器客户端 API&lt;/td>
&lt;td>kube-apiserver, etcd&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>TCP&lt;/td>
&lt;td>入站&lt;/td>
&lt;td>10250&lt;/td>
&lt;td>Kubelet API&lt;/td>
&lt;td>kubelet 自身、控制平面组件&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>TCP&lt;/td>
&lt;td>入站&lt;/td>
&lt;td>10251&lt;/td>
&lt;td>kube-scheduler&lt;/td>
&lt;td>kube-scheduler 自身&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>TCP&lt;/td>
&lt;td>入站&lt;/td>
&lt;td>10252&lt;/td>
&lt;td>kube-controller-manager&lt;/td>
&lt;td>kube-controller-manager 自身&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>工作节点的端口清单，如果有本机防火墙的话需要开放下面的端口。&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>协议&lt;/th>
&lt;th>方向&lt;/th>
&lt;th>端口范围&lt;/th>
&lt;th>作用&lt;/th>
&lt;th>使用者&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>TCP&lt;/td>
&lt;td>入站&lt;/td>
&lt;td>10250&lt;/td>
&lt;td>Kubelet API&lt;/td>
&lt;td>kubelet 自身、控制平面组件&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>TCP&lt;/td>
&lt;td>入站&lt;/td>
&lt;td>30000-32767&lt;/td>
&lt;td>NodePort 服务†&lt;/td>
&lt;td>所有组件&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="容器运行时">容器运行时&lt;/h3>
&lt;p>参考 &lt;a class="link" href="https://mirrors.tuna.tsinghua.edu.cn/help/docker-ce/" target="_blank" rel="noopener"
>清华大学开源软件镜像站 Docker Community Edition 镜像使用帮助&lt;/a>。&lt;/p>
&lt;h3 id="安装kubeadm">安装kubeadm&lt;/h3>
&lt;p>先信任软件仓库的证书，要注意的是证书托管在谷歌，所以基本不用考虑直接执行命令能成功了。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg &lt;span class="p">|&lt;/span> sudo apt-key add -
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>作为代替，可以先手动魔法上网下载到证书，再变通一下完成证书添加。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">cat apt-key.gpg &lt;span class="p">|&lt;/span> sudo apt-key add -
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>之后添加源，源的版本并不能直接对应到发行版的版本，目前 ubuntu server 只支持到 16.04 LTS ，或者 Debian 9 Stretch 。更高版本也可以装，但我比较怀疑官方的包到底有没有在新发行版里测试过，支持力度行不行。&lt;/p>
&lt;p>总之，如果宿主机不拿来当开发环境使的话，上个 Ubuntu server 16.04 LTS 也没事，只要还没有完全停止支持就好。总之这个问题上我保留意见吧。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="nb">echo&lt;/span> &lt;span class="s1">&amp;#39;deb https://mirrors.tuna.tsinghua.edu.cn/kubernetes/apt kubernetes-xenial main&amp;#39;&lt;/span> &lt;span class="p">|&lt;/span> sudo tee /etc/apt/sources.list.d/kubernetes.list
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>添加软件源之后更新软件包清单并安装 &lt;code>kubelet&lt;/code>、&lt;code>kubeadm&lt;/code>、&lt;code>kubectl&lt;/code> 。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="检查">检查&lt;/h3>
&lt;p>首先确认所有 &lt;code>kubelet&lt;/code>、&lt;code>kubeadm&lt;/code>、&lt;code>kubectl&lt;/code> 命令都已经可用，如果命令不存在则说明安装有问题，根据具体情况处理。&lt;/p>
&lt;p>然后检查&lt;code>kubelet&lt;/code>服务的状态（注意用了&lt;code>systemd&lt;/code>，不确定有没有用 &lt;code>upstart&lt;/code> 或别的 Unix 风格的服务管理的）。&lt;/p>
&lt;p>运行命令 &lt;code>sudo systemctl status kubelet&lt;/code> 得到下面的输出。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;span class="lnt">8
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-plaintext" data-lang="plaintext">● kubelet.service - kubelet: The Kubernetes Node Agent
Loaded: loaded (/lib/systemd/system/kubelet.service; enabled; vendor preset: enabled)
Drop-In: /etc/systemd/system/kubelet.service.d
└─10-kubeadm.conf
Active: activating (auto-restart) (Result: exit-code) since Fri 2021-11-19 02:32:29 UTC; 9s ago
Docs: https://kubernetes.io/docs/home/
Process: 6767 ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS (code=exited, status=1/FAILURE)
Main PID: 6767 (code=exited, status=1/FAILURE)
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>此时的 &lt;code>kubelet&lt;/code> 服务还是失败的状态，再检查 &lt;code>kubelet&lt;/code> 的日志，通过 &lt;code>sudo journalctl -u kubelet&lt;/code> 。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-plaintext" data-lang="plaintext">-- Logs begin at Thu 2021-11-18 07:40:58 UTC, end at Fri 2021-11-19 02:34:19 UTC. --
Nov 19 02:27:41 vm systemd[1]: Started kubelet: The Kubernetes Node Agent.
Nov 19 02:27:42 vm systemd[1]: kubelet.service: Current command vanished from the unit file, execution of the command list won&amp;#39;t be resumed.
Nov 19 02:27:42 vm systemd[1]: Stopping kubelet: The Kubernetes Node Agent...
Nov 19 02:27:42 vm systemd[1]: kubelet.service: Succeeded.
Nov 19 02:27:42 vm systemd[1]: Stopped kubelet: The Kubernetes Node Agent.
Nov 19 02:27:42 vm systemd[1]: Started kubelet: The Kubernetes Node Agent.
Nov 19 02:27:42 vm kubelet[5944]: E1119 02:27:42.559949 5944 server.go:206] &amp;#34;Failed to load kubelet config file&amp;#34; err=&amp;#34;failed to load Kubelet config file /var/lib/kubelet/config.yaml, error failed to read kube&amp;gt;
Nov 19 02:27:42 vm systemd[1]: kubelet.service: Main process exited, code=exited, status=1/FAILURE
Nov 19 02:27:42 vm systemd[1]: kubelet.service: Failed with result &amp;#39;exit-code&amp;#39;.
Nov 19 02:27:52 vm systemd[1]: kubelet.service: Scheduled restart job, restart counter is at 1.
Nov 19 02:27:52 vm systemd[1]: Stopped kubelet: The Kubernetes Node Agent.
Nov 19 02:27:52 vm systemd[1]: Started kubelet: The Kubernetes Node Agent.
Nov 19 02:27:52 vm kubelet[6119]: E1119 02:27:52.804723 6119 server.go:206] &amp;#34;Failed to load kubelet config file&amp;#34; err=&amp;#34;failed to load Kubelet config file /var/lib/kubelet/config.yaml, error failed to read kube&amp;gt;
Nov 19 02:27:52 vm systemd[1]: kubelet.service: Main process exited, code=exited, status=1/FAILURE
Nov 19 02:27:52 vm systemd[1]: kubelet.service: Failed with result &amp;#39;exit-code&amp;#39;.
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>失败的原因是 &lt;code>Failed to load kubelet config file&amp;quot; err=&amp;quot;failed to load Kubelet config file /var/lib/kubelet/config.yaml, error failed to read kube&amp;gt;&lt;/code>。&lt;/p>
&lt;h2 id="创建集群">创建集群&lt;/h2>
&lt;p>目标是创建一个单节点的集群。&lt;/p>
&lt;h3 id="拉取镜像">拉取镜像&lt;/h3>
&lt;p>众所周知的原因，&lt;code>kubernetes&lt;/code> 的镜像托管在谷歌服务器上，麻瓜是访问不到的，所以就连拉取镜像也值得用几十个字来说。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">sudo kubeadm config images pull --image-repository registry.cn-hangzhou.aliyuncs.com/google_containers
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="初始化主节点">初始化主节点&lt;/h3>
&lt;p>注意使用 &lt;code>kubeadm config images pull&lt;/code> 拉取了镜像的话，在 &lt;code>init&lt;/code> 阶段除非你把镜像 tag 给改了，不然也要传个 &lt;code>--image-repository&lt;/code> 参数。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">sudo kubeadm init --image-repository registry.cn-hangzhou.aliyuncs.com/google_containers
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>完整输出如下。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;span class="lnt">40
&lt;/span>&lt;span class="lnt">41
&lt;/span>&lt;span class="lnt">42
&lt;/span>&lt;span class="lnt">43
&lt;/span>&lt;span class="lnt">44
&lt;/span>&lt;span class="lnt">45
&lt;/span>&lt;span class="lnt">46
&lt;/span>&lt;span class="lnt">47
&lt;/span>&lt;span class="lnt">48
&lt;/span>&lt;span class="lnt">49
&lt;/span>&lt;span class="lnt">50
&lt;/span>&lt;span class="lnt">51
&lt;/span>&lt;span class="lnt">52
&lt;/span>&lt;span class="lnt">53
&lt;/span>&lt;span class="lnt">54
&lt;/span>&lt;span class="lnt">55
&lt;/span>&lt;span class="lnt">56
&lt;/span>&lt;span class="lnt">57
&lt;/span>&lt;span class="lnt">58
&lt;/span>&lt;span class="lnt">59
&lt;/span>&lt;span class="lnt">60
&lt;/span>&lt;span class="lnt">61
&lt;/span>&lt;span class="lnt">62
&lt;/span>&lt;span class="lnt">63
&lt;/span>&lt;span class="lnt">64
&lt;/span>&lt;span class="lnt">65
&lt;/span>&lt;span class="lnt">66
&lt;/span>&lt;span class="lnt">67
&lt;/span>&lt;span class="lnt">68
&lt;/span>&lt;span class="lnt">69
&lt;/span>&lt;span class="lnt">70
&lt;/span>&lt;span class="lnt">71
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-plaintext" data-lang="plaintext">[init] Using Kubernetes version: v1.22.4
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using &amp;#39;kubeadm config images pull&amp;#39;
[certs] Using certificateDir folder &amp;#34;/etc/kubernetes/pki&amp;#34;
[certs] Generating &amp;#34;ca&amp;#34; certificate and key
[certs] Generating &amp;#34;apiserver&amp;#34; certificate and key
[certs] apiserver serving cert is signed for DNS names [kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local vm] and IPs [10.96.0.1 10.0.2.15]
[certs] Generating &amp;#34;apiserver-kubelet-client&amp;#34; certificate and key
[certs] Generating &amp;#34;front-proxy-ca&amp;#34; certificate and key
[certs] Generating &amp;#34;front-proxy-client&amp;#34; certificate and key
[certs] Generating &amp;#34;etcd/ca&amp;#34; certificate and key
[certs] Generating &amp;#34;etcd/server&amp;#34; certificate and key
[certs] etcd/server serving cert is signed for DNS names [localhost vm] and IPs [10.0.2.15 127.0.0.1 ::1]
[certs] Generating &amp;#34;etcd/peer&amp;#34; certificate and key
[certs] etcd/peer serving cert is signed for DNS names [localhost vm] and IPs [10.0.2.15 127.0.0.1 ::1]
[certs] Generating &amp;#34;etcd/healthcheck-client&amp;#34; certificate and key
[certs] Generating &amp;#34;apiserver-etcd-client&amp;#34; certificate and key
[certs] Generating &amp;#34;sa&amp;#34; key and public key
[kubeconfig] Using kubeconfig folder &amp;#34;/etc/kubernetes&amp;#34;
[kubeconfig] Writing &amp;#34;admin.conf&amp;#34; kubeconfig file
[kubeconfig] Writing &amp;#34;kubelet.conf&amp;#34; kubeconfig file
[kubeconfig] Writing &amp;#34;controller-manager.conf&amp;#34; kubeconfig file
[kubeconfig] Writing &amp;#34;scheduler.conf&amp;#34; kubeconfig file
[kubelet-start] Writing kubelet environment file with flags to file &amp;#34;/var/lib/kubelet/kubeadm-flags.env&amp;#34;
[kubelet-start] Writing kubelet configuration to file &amp;#34;/var/lib/kubelet/config.yaml&amp;#34;
[kubelet-start] Starting the kubelet
[control-plane] Using manifest folder &amp;#34;/etc/kubernetes/manifests&amp;#34;
[control-plane] Creating static Pod manifest for &amp;#34;kube-apiserver&amp;#34;
[control-plane] Creating static Pod manifest for &amp;#34;kube-controller-manager&amp;#34;
[control-plane] Creating static Pod manifest for &amp;#34;kube-scheduler&amp;#34;
[etcd] Creating static Pod manifest for local etcd in &amp;#34;/etc/kubernetes/manifests&amp;#34;
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory &amp;#34;/etc/kubernetes/manifests&amp;#34;. This can take up to 4m0s
sudo kubeadm init --image-repository registry.cn-hangzhou.aliyuncs.com/google_containers[apiclient] All control plane components are healthy after 9.003038 seconds
[upload-config] Storing the configuration used in ConfigMap &amp;#34;kubeadm-config&amp;#34; in the &amp;#34;kube-system&amp;#34; Namespace
[kubelet] Creating a ConfigMap &amp;#34;kubelet-config-1.22&amp;#34; in namespace kube-system with the configuration for the kubelets in the cluster
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node vm as control-plane by adding the labels: [node-role.kubernetes.io/master(deprecated) node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node vm as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
[bootstrap-token] Using token: jfhacg.2ahc3yqndiwct9vk
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the &amp;#34;cluster-info&amp;#34; ConfigMap in the &amp;#34;kube-public&amp;#34; namespace
[kubelet-finalize] Updating &amp;#34;/etc/kubernetes/kubelet.conf&amp;#34; to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy
Your Kubernetes control-plane has initialized successfully!
To start using your cluster, you need to run the following as a regular user:
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
Alternatively, if you are the root user, you can run:
export KUBECONFIG=/etc/kubernetes/admin.conf
You should now deploy a pod network to the cluster.
Run &amp;#34;kubectl apply -f [podnetwork].yaml&amp;#34; with one of the options listed at:
https://kubernetes.io/docs/concepts/cluster-administration/addons/
Then you can join any number of worker nodes by running the following on each as root:
kubeadm join 10.0.2.15:6443 --token jfhacg.2ahc3yqndiwct9vk \
--discovery-token-ca-cert-hash sha256:377d6ead2bde8373000333d883c9bd9449233686fe277814ccade0b55fc362a1
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>因为是虚拟机里的集群，也没打算给任何人访问，关键信息懒得打码了。&lt;/p>
&lt;p>几个值得关注的内容：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="c1"># Your Kubernetes control-plane has initialized successfully!&lt;/span>
&lt;span class="c1"># To start using your cluster, you need to run the following as a regular user:&lt;/span>
mkdir -p &lt;span class="nv">$HOME&lt;/span>/.kube
sudo cp -i /etc/kubernetes/admin.conf &lt;span class="nv">$HOME&lt;/span>/.kube/config
sudo chown &lt;span class="k">$(&lt;/span>id -u&lt;span class="k">)&lt;/span>:&lt;span class="k">$(&lt;/span>id -g&lt;span class="k">)&lt;/span> &lt;span class="nv">$HOME&lt;/span>/.kube/config
&lt;span class="c1"># Alternatively, if you are the root user, you can run:&lt;/span>
&lt;span class="nb">export&lt;/span> &lt;span class="nv">KUBECONFIG&lt;/span>&lt;span class="o">=&lt;/span>/etc/kubernetes/admin.conf
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>首先，集群控制平面已经初始化成功了，说明命令执行基本 OK，没有致命错误。&lt;/p>
&lt;p>后面就是教你怎么配置 &lt;code>kubectl&lt;/code> 来访问控制平面，集群的管理员配置放在 &lt;code>/etc/kubernetes/admin.conf&lt;/code> ，可以用 &lt;code>KUBECONFIG&lt;/code> 环境变量来使用，或者把配置文件复制到家目录下的路径 &lt;code>~/.kube/config&lt;/code> 。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-plaintext" data-lang="plaintext">You should now deploy a pod network to the cluster.
Run &amp;#34;kubectl apply -f [podnetwork].yaml&amp;#34; with one of the options listed at:
https://kubernetes.io/docs/concepts/cluster-administration/addons/
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>提示你应该部署一个 POD 网络到集群，也就是一般说的 CNI 插件，以便 POD 之间可以互相通信。安装插件之前，集群的 DNS （&lt;code>CoreDNS&lt;/code>） 不会启动。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-plaintext" data-lang="plaintext">Then you can join any number of worker nodes by running the following on each as root:
kubeadm join 10.0.2.15:6443 --token jfhacg.2ahc3yqndiwct9vk \
--discovery-token-ca-cert-hash sha256:377d6ead2bde8373000333d883c9bd9449233686fe277814ccade0b55fc362a1
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>一旦搞定了网络插件，就可以用 &lt;code>kubeadm&lt;/code> 继续添加新的节点到集群里了。&lt;/p>
&lt;h3 id="安装网络插件">安装网络插件&lt;/h3>
&lt;p>看起来大家都在用 &lt;code>calico&lt;/code> 做 POD 网络，所以我也用 &lt;code>calico&lt;/code> 好了。步骤参考 &lt;code>calico&lt;/code> 的&lt;a class="link" href="https://docs.projectcalico.org/getting-started/kubernetes/self-managed-onprem/onpremises" target="_blank" rel="noopener"
>官方文档&lt;/a> 和 &lt;a class="link" href="https://docs.projectcalico.org/getting-started/kubernetes/quickstart" target="_blank" rel="noopener"
>官方的快速开始&lt;/a> 来配置一个单节点集群的 POD 。&lt;/p>
&lt;p>正式开始前，参考上面的内容配置好 &lt;code>kubectl&lt;/code> ，以便无需 root 权限运行 &lt;code>kubectl&lt;/code> 命令。&lt;/p>
&lt;p>先下载 &lt;code>calico&lt;/code> 的 k8s 资源。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">curl https://docs.projectcalico.org/manifests/calico-typha.yaml -o calico.yaml
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>按照说明，判断下 POD 的 CIDR（POD的网段），用 &lt;code>sudo kubectl --kubeconfig /etc/kubernetes/admin.conf get cm kubeadm-config -n kube-system -o yaml&lt;/code> 获取 &lt;code>kubeadm-config&lt;/code> 这个 &lt;code>configmap&lt;/code>，检查其中的 &lt;code>networking.podSubnet&lt;/code> 值。在我这里的输出如下。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-yaml" data-lang="yaml">&lt;span class="nt">apiVersion&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">v1&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w">&lt;/span>&lt;span class="nt">data&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">ClusterConfiguration&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">|&lt;/span>&lt;span class="sd">
&lt;/span>&lt;span class="sd"> apiServer:
&lt;/span>&lt;span class="sd"> extraArgs:
&lt;/span>&lt;span class="sd"> authorization-mode: Node,RBAC
&lt;/span>&lt;span class="sd"> timeoutForControlPlane: 4m0s
&lt;/span>&lt;span class="sd"> apiVersion: kubeadm.k8s.io/v1beta3
&lt;/span>&lt;span class="sd"> certificatesDir: /etc/kubernetes/pki
&lt;/span>&lt;span class="sd"> clusterName: kubernetes
&lt;/span>&lt;span class="sd"> controllerManager: {}
&lt;/span>&lt;span class="sd"> dns: {}
&lt;/span>&lt;span class="sd"> etcd:
&lt;/span>&lt;span class="sd"> local:
&lt;/span>&lt;span class="sd"> dataDir: /var/lib/etcd
&lt;/span>&lt;span class="sd"> imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers
&lt;/span>&lt;span class="sd"> kind: ClusterConfiguration
&lt;/span>&lt;span class="sd"> kubernetesVersion: v1.22.4
&lt;/span>&lt;span class="sd"> networking:
&lt;/span>&lt;span class="sd"> dnsDomain: cluster.local
&lt;/span>&lt;span class="sd"> serviceSubnet: 10.96.0.0/12
&lt;/span>&lt;span class="sd"> scheduler: {}&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w">&lt;/span>&lt;span class="nt">kind&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">ConfigMap&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w">&lt;/span>&lt;span class="nt">metadata&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">creationTimestamp&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s2">&amp;#34;2021-11-19T02:54:04Z&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">kubeadm-config&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">namespace&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">kube-system&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">resourceVersion&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s2">&amp;#34;210&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">uid&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">2567d366-2257-4114-8709-12b016cd1fe8&lt;/span>&lt;span class="w">
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>可以发现没有 &lt;code>podSubnet&lt;/code>，那就当是默认，按照 &lt;code>calico&lt;/code> 文档说明不用改 &lt;code>yaml&lt;/code>，正常应用。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">sudo kubectl --kubeconfig /etc/kubernetes/admin.conf apply -f calico.yaml
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>输出如下。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-plaintext" data-lang="plaintext">configmap/calico-config created
customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/caliconodestatuses.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipreservations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/kubecontrollersconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created
clusterrole.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrole.rbac.authorization.k8s.io/calico-node created
clusterrolebinding.rbac.authorization.k8s.io/calico-node created
service/calico-typha created
deployment.apps/calico-typha created
Warning: policy/v1beta1 PodDisruptionBudget is deprecated in v1.21+, unavailable in v1.25+; use policy/v1 PodDisruptionBudget
poddisruptionbudget.policy/calico-typha created
daemonset.apps/calico-node created
serviceaccount/calico-node created
deployment.apps/calico-kube-controllers created
serviceaccount/calico-kube-controllers created
poddisruptionbudget.policy/calico-kube-controllers created
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>出现了一个弃用警告，无视之，反正是 &lt;code>calico&lt;/code> 的问题。再检查下相关的 &lt;code>POD&lt;/code> 创建是否成功，用命令 &lt;code>kubectl get pod -n kube-system&lt;/code>，输出如下。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-plaintext" data-lang="plaintext">NAMESPACE NAME READY STATUS RESTARTS AGE
kube-system calico-kube-controllers-5d995d45d6-gwwrw 1/1 Running 0 5m29s
kube-system calico-node-sgb2x 0/1 Running 2 (49s ago) 5m29s
kube-system calico-typha-7df55cc78b-hpfkx 0/1 Pending 0 5m29s
kube-system coredns-7d89d9b6b8-c7sxl 1/1 Running 0 32m
kube-system coredns-7d89d9b6b8-tjsj8 1/1 Running 0 32m
kube-system etcd-vm 1/1 Running 0 32m
kube-system kube-apiserver-vm 1/1 Running 0 32m
kube-system kube-controller-manager-vm 1/1 Running 0 32m
kube-system kube-proxy-d64kh 1/1 Running 0 32m
kube-system kube-scheduler-vm 1/1 Running 0 32m
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>可以看到至少镜像是拉到了。&lt;/p>
&lt;p>&lt;code>calico-typha-7df55cc78b-hpfkx&lt;/code> 这个 POD 的 &lt;code>describe&lt;/code> 显示不能运行在 &lt;code>master&lt;/code> 节点。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-plaintext" data-lang="plaintext">Events:
Type Reason Age From Message
---- ------ ---- ---- -------
Warning FailedScheduling 56s (x9 over 8m57s) default-scheduler 0/1 nodes are available: 1 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn&amp;#39;t tolerate.
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>而 &lt;code>calico-node-sgb2x&lt;/code> 的日志显示需要 &lt;code>calico-typha&lt;/code> 才能运行。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-plaintext" data-lang="plaintext">2021-11-19 03:27:46.413 [ERROR][1674] confd/discovery.go 153: Didn&amp;#39;t find any ready Typha instances.
2021-11-19 03:27:46.413 [FATAL][1674] confd/startsyncerclient.go 48: Typha discovery enabled but discovery failed. error=Kubernetes service missing IP or port
bird: Unable to open configuration file /etc/calico/confd/config/bird6.cfg: No such file or directory
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>因为想要的是一个单节点集群，所以接下来把本节点的污点 &lt;code>node-role.kubernetes.io/master-&lt;/code> 给去掉。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">sudo kubectl --kubeconfig /etc/kubernetes/admin.conf taint nodes --all node-role.kubernetes.io/master-
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>输出&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-plaintext" data-lang="plaintext">node/vm untainted
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>再观察 &lt;code>kube-system&lt;/code> 里的 POD 状态。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-plaintext" data-lang="plaintext">NAME READY STATUS RESTARTS AGE
calico-kube-controllers-5d995d45d6-gwwrw 1/1 Running 0 11m
calico-node-sgb2x 1/1 Running 7 (38s ago) 11m
calico-typha-7df55cc78b-hpfkx 1/1 Running 0 11m
coredns-7d89d9b6b8-c7sxl 1/1 Running 0 38m
coredns-7d89d9b6b8-tjsj8 1/1 Running 0 38m
etcd-vm 1/1 Running 0 38m
kube-apiserver-vm 1/1 Running 0 38m
kube-controller-manager-vm 1/1 Running 0 38m
kube-proxy-d64kh 1/1 Running 0 38m
kube-scheduler-vm 1/1 Running 0 38m
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>可以看到所有的POD都已经进入&lt;code>Ready&lt;/code>状态。&lt;/p>
&lt;p>最后通过 &lt;code>kubectl get nodes -o wide&lt;/code> 检查节点状态。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-plaintext" data-lang="plaintext">NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME
vm Ready control-plane,master 39m v1.22.4 10.0.2.15 &amp;lt;none&amp;gt; Ubuntu 20.04.3 LTS 5.4.0-90-generic docker://20.10.11
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>到这里单节点集群就成功部署了。&lt;/p>
&lt;h2 id="总结">总结&lt;/h2>
&lt;p>之后还可以部署 dashboard 之类的应用验证，不想写了，浪费时间。&lt;/p>
&lt;p>写了一大堆又删掉了。&lt;/p>
&lt;p>如果一定要总结的话，k8s，学了进小厂吧，小厂不用；学了进大厂吧，大厂也不要你。&lt;/p></description></item><item><title>k3s更新客户端证书的偷懒方法</title><link>https://nnnewb.github.io/p/k3s-renew-client-ca-file-the-lazy-way/</link><pubDate>Mon, 11 Oct 2021 13:58:00 +0800</pubDate><guid>https://nnnewb.github.io/p/k3s-renew-client-ca-file-the-lazy-way/</guid><description>&lt;h2 id="前言">前言&lt;/h2>
&lt;p>今天上内网服务器看了眼，准备调试下新代码，结果发现报错 &lt;code>You must logged in to the server (unauthorized)&lt;/code> 。翻了半天的 &lt;em>KUBECONFIG&lt;/em> 配置，发现啥也没错。换成 &lt;code>/etc/rancher/k3s/k3s.yaml&lt;/code> 也不行。于是查了下 &lt;code>journalctl -r -u k3s&lt;/code> ，发现日志 &lt;code>x509: certificate has expired or not yet valid: current time ...&lt;/code> ，这就明确了是证书过期了。&lt;/p>
&lt;p>于是又找了一圈如何给k3s更新证书，搜 &lt;code>how to renew client-ca-file&lt;/code> 查出来的方法不是 &lt;code>kubeadm&lt;/code> 就是改时间、换证书，总之&amp;hellip;麻烦，而且搜出来的文章可操作性都有点差，真要实践出真知也不能放公司的机器上，搞出点问题还得劝自己心平气和磨上一整天去解决。&lt;/p>
&lt;p>于是终于找到个看起来能行的办法：重启。&lt;/p>
&lt;h2 id="操作">操作&lt;/h2>
&lt;p>这个办法可操作性很强——反正情况不会变得更差了。因为办公室的服务器并不能保证24小时不断电，有时候白天上班机器是关机的，重启k3s无论如何不会导致问题变得更差——就算放着不管，过两天说不定也会断电重启下。&lt;/p>
&lt;p>确认没人用服务之后直接上手。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">sudo systemctl restart k3s
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>等待重启完成，测试下新的 &lt;code>k3s.yaml&lt;/code> 能不能正常用。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="nb">export&lt;/span> &lt;span class="nv">KUBECONFIG&lt;/span>&lt;span class="o">=&lt;/span>/etc/rancher/k3s/k3s.yaml
kubectl cluster-info
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">Kubernetes control plane is running at https://192.168.2.175:6443
CoreDNS is running at https://192.168.2.175:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
Metrics-server is running at https://192.168.2.175:6443/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy
To further debug and diagnose cluster problems, use &amp;#39;kubectl cluster-info dump&amp;#39;.
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>其他 &lt;code>get nodes&lt;/code> 之类的命令也顺利完成，剩下就是把新的客户端证书合并到个人的配置里了（对，并不是直接用 &lt;code>/etc/rancher/k3s/k3s.yaml&lt;/code>，我知道有人会这么用）。办法也简单，&lt;code>vim /etc/rancher/k3s/k3s.yaml&lt;/code>，把里面的 &lt;code>users&lt;/code> 键下，&lt;code>default&lt;/code> 用户的信息复制出来，粘贴到个人的 &lt;code>~/.kube/config&lt;/code> 相应位置就好。以前复制过的话，就覆盖掉。&lt;/p>
&lt;h2 id="总结">总结&lt;/h2>
&lt;p>没啥好总结的，重启大法解决一切问题。不过手动轮换证书的办法也得记录一下，这里留相关的摘要链接。&lt;/p>
&lt;ul>
&lt;li>&lt;a class="link" href="https://kubernetes.io/zh/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/" target="_blank" rel="noopener"
>kubernetes.io/使用 kubeadm 进行证书管理&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://www.ibm.com/docs/en/fci/1.1.0?topic=kubernetes-renewing-cluster-certificates" target="_blank" rel="noopener"
>ibm.com/renewing kubernetes cluster certificates&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://forums.rancher.com/t/how-to-renew-cert-manually/20022" target="_blank" rel="noopener"
>forum.rancher.com/how to renew cert manually?&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>比较好奇的有多个master节点的集群，能通过逐个重启master节点来实现自动更新证书吗？&lt;/p></description></item><item><title>csr 方式创建 kubernetes 用户出了点差错</title><link>https://nnnewb.github.io/p/csr-%E6%96%B9%E5%BC%8F%E5%88%9B%E5%BB%BA-kubernetes-%E7%94%A8%E6%88%B7%E5%87%BA%E4%BA%86%E7%82%B9%E5%B7%AE%E9%94%99/</link><pubDate>Mon, 19 Jul 2021 09:52:38 +0800</pubDate><guid>https://nnnewb.github.io/p/csr-%E6%96%B9%E5%BC%8F%E5%88%9B%E5%BB%BA-kubernetes-%E7%94%A8%E6%88%B7%E5%87%BA%E4%BA%86%E7%82%B9%E5%B7%AE%E9%94%99/</guid><description>&lt;p>越是在 kubernetes 的浑水里摸索，越是发现这就是个不顺手的锤子。&lt;/p>
&lt;p>网上很多人喜欢把东西用不惯叫做懒，蠢，要是多反驳几句，那就还得搭上个“坏”的帽子。感觉吧，就这帮人看来，大神放个屁也值得学习，从里面“悟”出什么道理。&lt;/p>
&lt;p>这帮人就跟传教士一样，但凡说个不字，就是在亵渎他们的“大神”。可谓人类迷惑行为。&lt;/p>
&lt;p>好吧。技术别饭圈化行吗？&lt;/p>
&lt;p>你说尤大强吗？Richard Stallman 是不是值得尊敬？Google 是不是最好的技术公司？Android 天下无敌？&lt;/p>
&lt;p>然后全摆上神坛，挂上赛博天神的牌匾，插上网线一天 25 小时膜拜？&lt;/p>
&lt;p>这帮人哪天搞个崇拜互联网和计算机的教派，把冯·诺依曼奉为先知我都不奇怪。&lt;/p>
&lt;p>拜托，你们真的好怪欸。&lt;/p>
&lt;!-- more -->
&lt;h2 id="完整脚本">完整脚本&lt;/h2>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;span class="lnt">40
&lt;/span>&lt;span class="lnt">41
&lt;/span>&lt;span class="lnt">42
&lt;/span>&lt;span class="lnt">43
&lt;/span>&lt;span class="lnt">44
&lt;/span>&lt;span class="lnt">45
&lt;/span>&lt;span class="lnt">46
&lt;/span>&lt;span class="lnt">47
&lt;/span>&lt;span class="lnt">48
&lt;/span>&lt;span class="lnt">49
&lt;/span>&lt;span class="lnt">50
&lt;/span>&lt;span class="lnt">51
&lt;/span>&lt;span class="lnt">52
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="cp">#!/bin/bash -e
&lt;/span>&lt;span class="cp">&lt;/span>&lt;span class="c1">#&lt;/span>
&lt;span class="c1"># 创建用户 gitlab 并授予权限&lt;/span>
&lt;span class="c1">#&lt;/span>
&lt;span class="c1"># reference:&lt;/span>
&lt;span class="c1"># https://kubernetes.io/zh/docs/reference/access-authn-authz/certificate-signing-requests/#normal-user&lt;/span>
&lt;span class="c1"># if `gitlab` does not exists,&lt;/span>
&lt;span class="c1"># create csr and approve&lt;/span>
&lt;span class="k">if&lt;/span> ! kubectl get csr gitlab &amp;gt;/dev/null&lt;span class="p">;&lt;/span> &lt;span class="k">then&lt;/span>
&lt;span class="c1"># create credential&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="o">[&lt;/span> ! -f gitlab.csr &lt;span class="o">]&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="k">then&lt;/span>
openssl genrsa -out gitlab.key &lt;span class="m">2048&lt;/span>
openssl req -new -key gitlab.key -out gitlab.csr
&lt;span class="k">fi&lt;/span>
&lt;span class="nv">csr&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="k">$(&lt;/span>cat gitlab.csr &lt;span class="p">|&lt;/span> base64 &lt;span class="p">|&lt;/span> tr -d &lt;span class="s2">&amp;#34;\n&amp;#34;&lt;/span>&lt;span class="k">)&lt;/span>
cat &lt;span class="s">&amp;lt;&amp;lt;EOF | tee gitlab-csr.yaml
&lt;/span>&lt;span class="s">apiVersion: certificates.k8s.io/v1beta1
&lt;/span>&lt;span class="s">kind: CertificateSigningRequest
&lt;/span>&lt;span class="s">metadata:
&lt;/span>&lt;span class="s"> name: gitlab
&lt;/span>&lt;span class="s">spec:
&lt;/span>&lt;span class="s"> groups:
&lt;/span>&lt;span class="s"> - system:authenticated
&lt;/span>&lt;span class="s"> request: $csr
&lt;/span>&lt;span class="s"> signerName: kubernetes.io/kube-apiserver-client
&lt;/span>&lt;span class="s"> usages:
&lt;/span>&lt;span class="s"> - client auth
&lt;/span>&lt;span class="s">EOF&lt;/span>
kubectl create -f gitlab-csr.yaml
kubectl certificate approve gitlab
&lt;span class="k">fi&lt;/span>
&lt;span class="c1"># get signed credential&lt;/span>
kubectl get csr gitlab -o &lt;span class="nv">jsonpath&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s1">&amp;#39;{.status.certificate}&amp;#39;&lt;/span>&lt;span class="p">|&lt;/span> base64 -d &amp;gt; gitlab.crt
&lt;span class="c1"># create role and rolebinding&lt;/span>
kubectl create role gitlab-ci &lt;span class="se">\
&lt;/span>&lt;span class="se">&lt;/span> --verb&lt;span class="o">=&lt;/span>create &lt;span class="se">\
&lt;/span>&lt;span class="se">&lt;/span> --verb&lt;span class="o">=&lt;/span>git &lt;span class="se">\
&lt;/span>&lt;span class="se">&lt;/span> --verb&lt;span class="o">=&lt;/span>list &lt;span class="se">\
&lt;/span>&lt;span class="se">&lt;/span> --verb&lt;span class="o">=&lt;/span>update &lt;span class="se">\
&lt;/span>&lt;span class="se">&lt;/span> --verb&lt;span class="o">=&lt;/span>delete &lt;span class="se">\
&lt;/span>&lt;span class="se">&lt;/span> --resource&lt;span class="o">=&lt;/span>pods &lt;span class="se">\
&lt;/span>&lt;span class="se">&lt;/span> --resource&lt;span class="o">=&lt;/span>deployment &lt;span class="se">\
&lt;/span>&lt;span class="se">&lt;/span> --resource&lt;span class="o">=&lt;/span>statefulset &lt;span class="se">\
&lt;/span>&lt;span class="se">&lt;/span> --resource&lt;span class="o">=&lt;/span>service &lt;span class="se">\
&lt;/span>&lt;span class="se">&lt;/span> --resource&lt;span class="o">=&lt;/span>configmap
kubectl create rolebinding gitlab-ci-binding-gitlab --role&lt;span class="o">=&lt;/span>gitlab-ci --user&lt;span class="o">=&lt;/span>gitlab
kubectl config set-credentials gitlab --client-key&lt;span class="o">=&lt;/span>gitlab.key --client-certificate&lt;span class="o">=&lt;/span>gitlab.crt --embed-certs&lt;span class="o">=&lt;/span>&lt;span class="nb">true&lt;/span>
kubectl config set-context ci --cluster&lt;span class="o">=&lt;/span>office --user&lt;span class="o">=&lt;/span>gitlab --namespace&lt;span class="o">=&lt;/span>version4
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="存在的问题">存在的问题&lt;/h2>
&lt;p>脚本跑完后发现还不能使用 &lt;code>kubectl get pods&lt;/code>，错误 Unauthorized。&lt;/p>
&lt;p>再看了一遍文档，发现有这么一句。&lt;/p>
&lt;blockquote>
&lt;p>下面的脚本展示了如何生成 PKI 私钥和 CSR。 设置 CSR 的 CN 和 O 属性很重要。CN 是用户名，O 是该用户归属的组。 你可以参考 RBAC 了解标准组的信息。&lt;/p>
&lt;/blockquote>
&lt;p>顺着链接去看了下 RBAC，结果也没找到什么“标准组”。&lt;/p>
&lt;p>对于文中说的两个“很重要”的字段，CN 我猜测是 Common Name，O 就是 Organization。现在就不知道怎么填 O，行吧。&lt;/p>
&lt;p>等啥时候搞清楚了再补一篇。&lt;/p></description></item></channel></rss>