<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>linux on weakptr's 笔记</title><link>https://nnnewb.github.io/blog/tags/linux/</link><description>Recent content in linux on weakptr's 笔记</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><lastBuildDate>Tue, 12 Dec 2023 19:08:56 +0800</lastBuildDate><atom:link href="https://nnnewb.github.io/blog/tags/linux/index.xml" rel="self" type="application/rss+xml"/><item><title>一套Linux网络开发/调试/运维的三板斧</title><link>https://nnnewb.github.io/blog/p/%E4%B8%80%E5%A5%97linux%E7%BD%91%E7%BB%9C%E5%BC%80%E5%8F%91/%E8%B0%83%E8%AF%95/%E8%BF%90%E7%BB%B4%E7%9A%84%E4%B8%89%E6%9D%BF%E6%96%A7/</link><pubDate>Tue, 12 Dec 2023 19:08:56 +0800</pubDate><guid>https://nnnewb.github.io/blog/p/%E4%B8%80%E5%A5%97linux%E7%BD%91%E7%BB%9C%E5%BC%80%E5%8F%91/%E8%B0%83%E8%AF%95/%E8%BF%90%E7%BB%B4%E7%9A%84%E4%B8%89%E6%9D%BF%E6%96%A7/</guid><description>&lt;img src="https://nnnewb.github.io/blog/p/%E4%B8%80%E5%A5%97linux%E7%BD%91%E7%BB%9C%E5%BC%80%E5%8F%91/%E8%B0%83%E8%AF%95/%E8%BF%90%E7%BB%B4%E7%9A%84%E4%B8%89%E6%9D%BF%E6%96%A7/cover.jpg" alt="Featured image of post 一套Linux网络开发/调试/运维的三板斧" />&lt;h2 id="前言">前言&lt;/h2>
&lt;p>作为网络领域的一个菜鸡，谈不了什么深入的东西。&lt;/p>
&lt;p>近两年的工作里接触比较多的，容器编排工具如k8s、compose，虚拟机编排工具如 libvirt、openstack、ESXi 这些玩意儿，多少都有网络层的虚拟化和编排能力。而我负责做蜜罐系统的主机防火墙策略、蜜罐网络编排、流量牵引，不可避免就会碰到很多网络问题。&lt;/p>
&lt;p>本文不能算正经技术分享文，就当成我对着空气和幻想中的朋友闲聊吧。梳理下从入职到离职这一年多以来，积累的一些技术实践，为建立知识体系做准备，先把点连成线。&lt;/p>
&lt;h2 id="三板斧之-tcpdumpwireshark">三板斧之 tcpdump/wireshark&lt;/h2>
&lt;p>&lt;code>tcpdump&lt;/code> 是最管用的一板斧，跟电工手里的万用表一样。调试防火墙策略、网络转发的时候，常见的异常表现就是没收到/没发出/连接异常，这些都可以通过 &lt;code>tcpdump&lt;/code> 排查。&lt;/p>
&lt;p>比如某个 &lt;code>docker-compose&lt;/code> 编排的业务服务容器，暴露方式是 &lt;code>docker-compose.yaml&lt;/code> 的 &lt;code>ports&lt;/code> 配置，自定义 docker 网络。问题表现是浏览器访问超时，反向代理没有 access log。主机没有开启 firewalld/ufw，selinux 已关闭，有自定义防火墙加固策略。那么问题的阻塞点在哪儿？&lt;/p>
&lt;p>&lt;code>tcpdump&lt;/code> 这时候就跟万用表一样出场了。先看看物理网卡的链路收到流量了没？&lt;code>tcpdump -i eno3 tcp and port 443 -nn&lt;/code>，哦吼，根本没收到。所以问题不在 我们服务器上，直接推给客户网管排查。&lt;/p>
&lt;p>再例如，改了个 &lt;code>libvirtd&lt;/code> 的配置后重启了一下 &lt;code>libvirtd&lt;/code>，发现虚拟机全部不联网了，虚拟机硬件配置无异常。怎么办？&lt;code>tcpdump&lt;/code> 看下网桥流量，咦，没有。再看下桥接，哦，虚拟机 tap 网卡 &lt;code>vnet*&lt;/code> 怎么全都断开桥接了？重新 &lt;code>brctl addif br-test vnet1&lt;/code> 接上，问题解决。再看是否是改配置重启 &lt;code>libvirtd&lt;/code> 的影响，加上对应处理。&lt;/p>
&lt;p>&lt;code>tcpdump&lt;/code> 和 &lt;code>wireshark&lt;/code> 结合使用效果更好，主要是 &lt;code>tcpdump&lt;/code> 分析流量内容没 &lt;code>wireshark&lt;/code> 简单直观。前公司的流量牵引功能实现里，有个用 go 写的的低效软件 NAT 和 tun 网卡，调试过程就会需要看 TCP 报头的字段在转发过程里变化，偶尔也需要看报文内容。&lt;code>tcpdump&lt;/code> 抓好的包还是用 &lt;code>wireshark&lt;/code> 分析更方便。&lt;/p>
&lt;h2 id="三板斧之-iptables">三板斧之 iptables&lt;/h2>
&lt;p>以我有限的经验来看，Linux 网络防火墙基本都包含一定程度上对 iptables 的接管。不论是 &lt;code>firewalld&lt;/code> 还是 &lt;code>ufw&lt;/code>，乃至国产麒麟桌面版自带的防火墙。麒麟桌面有两套防火墙，&lt;code>ufw&lt;/code> 和一套没有提供 cli 的防火墙 &lt;code>kylin-firewall&lt;/code>，就是配置在 &lt;code>/etc/kylin-firewall&lt;/code> 里的那套玩意儿。服务器版则只有 &lt;code>firewalld&lt;/code>。桌面版和服务器版来源大概一个是ubuntu一个是centos。&lt;/p>
&lt;p>&lt;code>iptables&lt;/code> 作为 Linux 防火墙技术的事实标准是必学的。工作常用的 &lt;code>docker&lt;/code> 也好，&lt;code>libvirt&lt;/code> 也好，默认都会涉及一些 &lt;code>iptables&lt;/code> 控制。&lt;/p>
&lt;p>&lt;code>iptables&lt;/code> 最让人烦的就是如果有多个程序想搞 iptables 策略，程序本身写得还不太好的时候，很容易导致策略顺序错乱。而 &lt;code>iptables&lt;/code> 策略对顺序又是敏感的。像 &lt;code>docker&lt;/code> 一样建一条用户策略链是个很好的选择，程序只需要确保用户链策略的存在性和相对顺序，内置链的跳转策略只需要要求存在。但 &lt;code>libvirt&lt;/code> 有点粗暴，策略直接写内建链里，相对就容易出毛病。&lt;/p>
&lt;p>在前司工作的时候，设计防火墙策略时就考虑了大家伙儿一起操作 &lt;code>iptables&lt;/code> 对策略顺序的影响，而且软件化、云部署等客户已有环境上部署的复杂场景，要求接管 &lt;code>iptables&lt;/code> 不太现实。所以仅做了一些有限的控制。比如，要求 &lt;code>docker-compose.yaml&lt;/code> 不配置 &lt;code>ports&lt;/code> 端口映射，因为 &lt;code>docker-proxy&lt;/code> 常出毛病。&lt;code>docker&lt;/code> 不接管 &lt;code>iptables&lt;/code>，业务容器 IP 采用静态定义+&lt;code>iptables&lt;/code>主动控制端口映射。&lt;code>libvirtd&lt;/code>也是，网络策略尽可能选择了自行接管，降低协作的复杂度。而这个决策的 tradeoff ，评估认为让 &lt;code>docker&lt;/code> + &lt;code>libvirtd&lt;/code> + 我们的管理服务 + 防火墙 协作管理 &lt;code>iptables&lt;/code> 的成本收益比太低。&lt;/p>
&lt;p>接管 &lt;code>docker&lt;/code> 和 &lt;code>libvirtd&lt;/code> 的策略还算好推，至于宿主机的防火墙，由于确实有客户在乎这个点（可能是内审合规要求？），所以接管后的策略还是以用户链的形式配置的。客户如果想保留防火墙协同管理 &lt;code>iptables&lt;/code> 策略，也可以，客户自行配置下防火墙的策略就好啦。&lt;/p>
&lt;p>另外几个 &lt;code>iptables&lt;/code> 的坑值得一提。&lt;/p>
&lt;p>一个是 &lt;code>iptables&lt;/code> 的 &lt;code>LOG&lt;/code> 目标和 &lt;code>TRACE&lt;/code> 目标打不出日志，&lt;code>dmesg&lt;/code> 啥也看不到。可能是没加载 &lt;code>nf_log_ipv4&lt;/code> 模块。&lt;code>modprobe nf_log_ipv4&lt;/code> 加载下就行。&lt;/p>
&lt;p>还有 &lt;code>iptables&lt;/code> 的内置表、链或目标不存在，比如 &lt;code>iptables -t nat -S&lt;/code> 提示没有 &lt;code>nat&lt;/code> 表，原因可能是 &lt;code>iptables&lt;/code> 安装损坏了，内核模块丢失。可以尝试重装。需要注意 &lt;code>iptables&lt;/code> 的内核模块包含在哪个包里。而目标不存在则考虑下是不是 &lt;code>iptables&lt;/code> 版本太低了。SLES 11 SP4 这个老古董发行版就缺很多目标。&lt;/p>
&lt;h2 id="三板斧之-iproute2">三板斧之 iproute2&lt;/h2>
&lt;p>&lt;code>iproute2&lt;/code> 是一套网络工具，是 &lt;code>ifconfig&lt;/code>、&lt;code>brctl&lt;/code>、&lt;code>netstat&lt;/code>、&lt;code>route&lt;/code> 这套 &lt;code>net-tools&lt;/code> 工具的替代。现在 &lt;code>ifconfig&lt;/code> 这套命令行工具是弃用状态，很多发行版较新版本要么不带 &lt;code>net-tools&lt;/code> 要么就是 &lt;code>net-tools&lt;/code> 和 &lt;code>iproute2&lt;/code> 共存了。&lt;/p>
&lt;p>&lt;code>iproute2&lt;/code> 这个包最主要用的工具还是 &lt;code>ip&lt;/code> ，用来调链路属性（UP/DOWN等）、IP地址、路由表和策略路由、ARP、隧道等。还有大伙儿应该听过的 &lt;code>ss&lt;/code> ，&lt;code>netstat&lt;/code> 的替代，以及个人用的比较少的 &lt;code>bridge&lt;/code> 。&lt;/p>
&lt;p>&lt;code>iproute2&lt;/code> 这套工具都是基于 &lt;code>netlink&lt;/code> 协议和内核通信的，用 go 写网络代码应该对 &lt;code>github.com/vishvananda/netlink&lt;/code> 这个包不陌生，很多 &lt;code>iproute2&lt;/code> 的功能可以在这个包里找到对应的 API 。&lt;/p>
&lt;h2 id="内核参数">内核参数&lt;/h2>
&lt;p>常用的内核参数列一下。&lt;/p>
&lt;p>&lt;code>net.ipv4.ip_forward&lt;/code> ，控制是否允许跨网卡的IP报文转发，或者简单点说就是路由功能。修改这个配置会影响其他配置，所以还是用 &lt;code>net.ipv4.conf.all.forwarding&lt;/code> 更好。&lt;code>net.ipv6.conf.all.forwarding&lt;/code> 是对应参数的 IPv6 版本。&lt;/p>
&lt;p>&lt;code>net.ipv4.conf.all.rp_filter&lt;/code>，如果反向路由校验不通过则丢弃包，也是在多网卡环境下有影响。举例来说，网卡 eno4 配置的 IP 是 &lt;code>172.19.0.1/24&lt;/code>，但 eno4 收到了来自 &lt;code>192.168.1.100&lt;/code> 的报文，系统没有针对这个 IP 的路由，而且 eno4 没有默认路由，返程会走另一个网卡。这种情况下就会丢弃报文而不处理。&lt;/p>
&lt;h2 id="networkmanager-和其他">NetworkManager 和其他&lt;/h2>
&lt;p>一些经验技巧性的东西。&lt;/p>
&lt;p>现在常见的 RHEL 系发行版和基于 RHEL 系发行版衍生的“兼容”、“自主”发行版基本都用的 &lt;code>NetworkManager&lt;/code>，坚持不把 &lt;code>NetworkManager&lt;/code> 设为默认的，主流发行版里除了 Arch Linux 这样让你自己选的之外，应该就剩 Debian 了。其他更小众的不谈。至于商用的，SLES、RHEL 都是默认 &lt;code>NetworkManager&lt;/code> 。学会用 &lt;code>NetworkManager&lt;/code> 还是很有必要的。&lt;/p>
&lt;p>&lt;code>NetworkManager&lt;/code> 的主要命令行交互界面是 &lt;code>nmcli&lt;/code> 命令，具体翻文档。给网络配置功能做图形前端主要用 &lt;code>NetworkManager&lt;/code> 的 D-Bus 接口。D-Bus 是个非常恶心的玩意儿，但目前没有其他替代，主流 Linux 服务适配的还是它。特别是 systemd 那套东西。有个已知的情况是在 systemd 系统上配 sys-v 启动脚本，有概率在 journald 采集到 systemd 启动的 sys-v 服务相关的 D-Bus 错误 （忘记具体错误消息是啥了，队列满什么的吧。）。安全行业客户日志审计遇到 error 都要我们给个解释，很难顶。专门去学 D-Bus 又很傻逼，ROI 太低。&lt;/p>
&lt;p>还有专门提一嘴的，APUE 这书真的值得手边参考。很多 *nix 常见编程范式都囊括了。读没读过这书做出来的程序设计真的会很不一样。&lt;/p>
&lt;h2 id="总结">总结&lt;/h2>
&lt;p>后面再想想归纳下这两年的工作经历，项目经验和教训。&lt;/p></description></item><item><title>VirtualBox 安装 32 位 CentOS 7.9 提示 Spurious ACK 问题</title><link>https://nnnewb.github.io/blog/p/virtualbox-%E5%AE%89%E8%A3%85-32-%E4%BD%8D-centos-7.9-%E6%8F%90%E7%A4%BA-spurious-ack-%E9%97%AE%E9%A2%98/</link><pubDate>Thu, 31 Aug 2023 13:40:44 +0800</pubDate><guid>https://nnnewb.github.io/blog/p/virtualbox-%E5%AE%89%E8%A3%85-32-%E4%BD%8D-centos-7.9-%E6%8F%90%E7%A4%BA-spurious-ack-%E9%97%AE%E9%A2%98/</guid><description>&lt;h2 id="前言">前言&lt;/h2>
&lt;p>为啥要装32位的？如果要问这问题我只能答你猜我乐不乐意装？&lt;/p>
&lt;p>工作需要没办法。&lt;/p>
&lt;h2 id="问题描述">问题描述&lt;/h2>
&lt;p>VirtualBox 版本 7.1.0 r158379 。&lt;/p>
&lt;p>宿主机 Windows 10.0.18363.1556 x64 。&lt;/p>
&lt;p>如果直接创建虚拟机后运行会出现&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">atkbd_interrupt: 2834 callbacks suppressed
atkbd serio0: Spurious ACK on isa0060/serio0. some program might trying to access hardware directly.
atkbd serio0: Spurious ACK on isa0060/serio0. some program might trying to access hardware directly.
...
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>然后在这个界面卡住。顺便我还注意到键盘的CAPS LOCK、SCROLL LOCK、NUM LOCK 三个灯在疯狂闪烁。&lt;/p>
&lt;h2 id="解决方法">解决方法&lt;/h2>
&lt;p>参考一个别人发给我的可以用的32位虚拟机配置，反复检查和对比实验后发现应该是虚拟机设置中的 &lt;code>I/O APIC&lt;/code> 影响。&lt;/p>
&lt;p>&lt;img src="https://nnnewb.github.io/blog/blog/p/virtualbox-%E5%AE%89%E8%A3%85-32-%E4%BD%8D-centos-7.9-%E6%8F%90%E7%A4%BA-spurious-ack-%E9%97%AE%E9%A2%98/image-20230831135203249.png"
width="713"
height="459"
srcset="https://nnnewb.github.io/blog/blog/p/virtualbox-%E5%AE%89%E8%A3%85-32-%E4%BD%8D-centos-7.9-%E6%8F%90%E7%A4%BA-spurious-ack-%E9%97%AE%E9%A2%98/image-20230831135203249_hu37dc36aeebdcd0d8c5f1f910faf9177b_20715_480x0_resize_box_3.png 480w, https://nnnewb.github.io/blog/blog/p/virtualbox-%E5%AE%89%E8%A3%85-32-%E4%BD%8D-centos-7.9-%E6%8F%90%E7%A4%BA-spurious-ack-%E9%97%AE%E9%A2%98/image-20230831135203249_hu37dc36aeebdcd0d8c5f1f910faf9177b_20715_1024x0_resize_box_3.png 1024w"
loading="lazy"
alt="image-20230831135203249"
class="gallery-image"
data-flex-grow="155"
data-flex-basis="372px"
>&lt;/p>
&lt;p>开启 &lt;code>I/O APIC&lt;/code> 会导致这个奇诡的现象。关闭 &lt;code>I/O APIC&lt;/code> 后能正常从 CentOS 7.9 i386 的 DVD 镜像进入安装界面，但关闭后只能分配一个 CPU 核心。尝试了调整其他选项，关掉了 USB 控制器、移除了所有网卡、没有串口、没有声卡，基本能关的都关了，只要开着 &lt;code>I/O APIC&lt;/code> 就会出现这个问题。系统安装前、安装后都不行。顺便一提我也试过 VMWare Workstation Pro 创建虚拟机，问题表现和 VirtualBox 差不多。但不太清楚 VMWare 的 &lt;code>I/O APIC&lt;/code> 开关在哪儿。&lt;/p>
&lt;p>怀疑是内核原因，毕竟就是卡在内核启动过程中。尝试装完后升级内核（升级后版本 &lt;code>3.10.0-1160.62.1.el7.centos.plus.i686&lt;/code>，升级前 &lt;code>3.10.0-1160.2.2.el7.centos.plus.i686&lt;/code>）再开启 &lt;code>I/O APIC&lt;/code> 就没问题啦。&lt;/p>
&lt;h2 id="总结">总结&lt;/h2>
&lt;p>遇到此问题可以尝试在创建完虚拟机后关掉 &lt;code>I/O APIC&lt;/code> ，启动虚拟机安装系统，装完再打开 &lt;code>I/O APIC&lt;/code>。&lt;/p></description></item><item><title>systemd 配置 ssh-agent 用户服务自启</title><link>https://nnnewb.github.io/blog/p/systemd-%E9%85%8D%E7%BD%AE-ssh-agent-%E7%94%A8%E6%88%B7%E6%9C%8D%E5%8A%A1%E8%87%AA%E5%90%AF/</link><pubDate>Mon, 27 Jun 2022 11:17:00 +0800</pubDate><guid>https://nnnewb.github.io/blog/p/systemd-%E9%85%8D%E7%BD%AE-ssh-agent-%E7%94%A8%E6%88%B7%E6%9C%8D%E5%8A%A1%E8%87%AA%E5%90%AF/</guid><description>&lt;h2 id="前言">前言&lt;/h2>
&lt;p>主要是想解决一个问题：ssh 只自动尝试了 &lt;code>~/.ssh/id_ed25519&lt;/code> 这个硬编码的路径，但我有两个 ed25519 秘钥（工作用一个，私人一个），除非用 &lt;code>ssh -i&lt;/code> 指定不然不会被自动发现和使用。&lt;/p>
&lt;p>但我又不想多打个 &lt;code>-i ~/.ssh/id_ed25519.xxx&lt;/code> ，所以就想配个 &lt;code>ssh-agent&lt;/code> 好了，手动&lt;code>ssh-add&lt;/code> 还是自动都可。&lt;/p>
&lt;h2 id="配置">配置&lt;/h2>
&lt;h3 id="创建服务配置">创建服务配置&lt;/h3>
&lt;p>位置：&lt;code>~/.config/systemd/user/ssh-agent.service&lt;/code>&lt;/p>
&lt;p>内容&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-ini" data-lang="ini">&lt;span class="k">[Unit]&lt;/span>
&lt;span class="na">Description&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s">SSH key agent&lt;/span>
&lt;span class="k">[Service]&lt;/span>
&lt;span class="na">Type&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s">simple&lt;/span>
&lt;span class="na">Environment&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s">SSH_AUTH_SOCK=%t/ssh-agent.socket&lt;/span>
&lt;span class="na">ExecStart&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s">/usr/bin/ssh-agent -D -a $SSH_AUTH_SOCK&lt;/span>
&lt;span class="k">[Install]&lt;/span>
&lt;span class="na">WantedBy&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s">default.target&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>添加 &lt;code>SSH_AUTH_SOCK DEFAULT=&amp;quot;${XDG_RUNTIME_DIR}/ssh-agent.socket&amp;quot;&lt;/code> 到 &lt;code>~/.pam_environment&lt;/code>。&lt;/p>
&lt;p>在我的系统上 &lt;code>XDG_RUNTIME_DIR&lt;/code> 对应 &lt;code>/run/user/你的用户id&lt;/code> ，不同发行版自己看下这个全局变量对应哪个位置。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="nb">echo&lt;/span> SSH_AUTH_SOCK &lt;span class="nv">DEFAULT&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="si">${&lt;/span>&lt;span class="nv">XDG_RUNTIME_DIR&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">/ssh-agent.socket&amp;#34;&lt;/span> &lt;span class="p">|&lt;/span> tee -a ~/.pam_environment
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>可选，自动添加秘钥（OpenSSH版本&amp;gt;=7.2）：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="nb">echo&lt;/span> &lt;span class="s1">&amp;#39;AddKeysToAgent yes&amp;#39;&lt;/span> &amp;gt;&amp;gt; ~/.ssh/config
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="启用服务">启用服务&lt;/h3>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">systemctl --user &lt;span class="nb">enable&lt;/span> ssh-agent
systemctl --user start ssh-agent
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>重新登录后生效。&lt;/p>
&lt;h2 id="总结">总结&lt;/h2>
&lt;p>参考：&lt;a class="link" href="https://unix.stackexchange.com/questions/339840/how-to-start-and-use-ssh-agent-as-systemd-service" target="_blank" rel="noopener"
>How to start and use ssh-agent as systemd service?&lt;/a>&lt;/p></description></item><item><title>kubeadm安装实验集群记录</title><link>https://nnnewb.github.io/blog/p/kubernetes-manually-install-by-kubeadm/</link><pubDate>Thu, 25 Nov 2021 14:31:00 +0800</pubDate><guid>https://nnnewb.github.io/blog/p/kubernetes-manually-install-by-kubeadm/</guid><description>&lt;h2 id="前言">前言&lt;/h2>
&lt;p>好吧，如果仔细想想就会发现不管是 k3s 还是 ucloud 上的 k8s ，都没有一个是自己手动配置好的。虽说并不是至关重要的，但手动用 kubeadm 装一次 kubernetes 总不会有什么坏处。顺手做个笔记。参考资料列出如下。&lt;/p>
&lt;ul>
&lt;li>&lt;a class="link" href="https://kubernetes.io/zh/docs/setup/production-environment/tools/kubeadm/install-kubeadm/" target="_blank" rel="noopener"
>https://kubernetes.io/zh/docs/setup/production-environment/tools/kubeadm/install-kubeadm/&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://mirrors.tuna.tsinghua.edu.cn/help/kubernetes/" target="_blank" rel="noopener"
>https://mirrors.tuna.tsinghua.edu.cn/help/kubernetes/&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://computingforgeeks.com/deploy-kubernetes-cluster-on-ubuntu-with-kubeadm/" target="_blank" rel="noopener"
>https://computingforgeeks.com/deploy-kubernetes-cluster-on-ubuntu-with-kubeadm/&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="系统配置">系统配置&lt;/h2>
&lt;p>正式安装之前先确认一些系统级配置。&lt;/p>
&lt;h3 id="swapoff">swapoff&lt;/h3>
&lt;p>简单的做法是 &lt;code>sudo swapoff -a&lt;/code> 即可。之后改 &lt;code>fstab&lt;/code> 把 &lt;code>swap&lt;/code> 分区关掉。&lt;/p>
&lt;h3 id="iptables检查桥接流量">iptables检查桥接流量&lt;/h3>
&lt;p>用 &lt;code>lsmod | grep bf_netfitler&lt;/code> 检查有没有启用 &lt;code>bf_netfilter&lt;/code> 模块，如果没有输出的话说明没加载，执行下面的命令。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">cat &lt;span class="s">&amp;lt;&amp;lt;EOF | sudo tee /etc/modules-load.d/k8s.conf
&lt;/span>&lt;span class="s">br_netfilter
&lt;/span>&lt;span class="s">EOF&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>会在 &lt;code>/etc/modules-load.d&lt;/code> 下添加一个模块自动加载的配置。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">cat &lt;span class="s">&amp;lt;&amp;lt;EOF | sudo tee /etc/sysctl.d/k8s.conf
&lt;/span>&lt;span class="s">net.bridge.bridge-nf-call-ip6tables = 1
&lt;/span>&lt;span class="s">net.bridge.bridge-nf-call-iptables = 1
&lt;/span>&lt;span class="s">EOF&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>再在 &lt;code>/etc/sysctl.d/&lt;/code> 下添加一个配置，允许 &lt;code>iptables&lt;/code> 查看桥接流量。&lt;/p>
&lt;p>然后用 &lt;code>sysctl&lt;/code> 重载配置。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">sudo sysctl --system
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="端口">端口&lt;/h3>
&lt;p>控制平面节点的端口清单，如果有本机防火墙的话需要开放下面的端口。&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>协议&lt;/th>
&lt;th>方向&lt;/th>
&lt;th>端口范围&lt;/th>
&lt;th>作用&lt;/th>
&lt;th>使用者&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>TCP&lt;/td>
&lt;td>入站&lt;/td>
&lt;td>6443&lt;/td>
&lt;td>Kubernetes API 服务器&lt;/td>
&lt;td>所有组件&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>TCP&lt;/td>
&lt;td>入站&lt;/td>
&lt;td>2379-2380&lt;/td>
&lt;td>etcd 服务器客户端 API&lt;/td>
&lt;td>kube-apiserver, etcd&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>TCP&lt;/td>
&lt;td>入站&lt;/td>
&lt;td>10250&lt;/td>
&lt;td>Kubelet API&lt;/td>
&lt;td>kubelet 自身、控制平面组件&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>TCP&lt;/td>
&lt;td>入站&lt;/td>
&lt;td>10251&lt;/td>
&lt;td>kube-scheduler&lt;/td>
&lt;td>kube-scheduler 自身&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>TCP&lt;/td>
&lt;td>入站&lt;/td>
&lt;td>10252&lt;/td>
&lt;td>kube-controller-manager&lt;/td>
&lt;td>kube-controller-manager 自身&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>工作节点的端口清单，如果有本机防火墙的话需要开放下面的端口。&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>协议&lt;/th>
&lt;th>方向&lt;/th>
&lt;th>端口范围&lt;/th>
&lt;th>作用&lt;/th>
&lt;th>使用者&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>TCP&lt;/td>
&lt;td>入站&lt;/td>
&lt;td>10250&lt;/td>
&lt;td>Kubelet API&lt;/td>
&lt;td>kubelet 自身、控制平面组件&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>TCP&lt;/td>
&lt;td>入站&lt;/td>
&lt;td>30000-32767&lt;/td>
&lt;td>NodePort 服务†&lt;/td>
&lt;td>所有组件&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="容器运行时">容器运行时&lt;/h3>
&lt;p>参考 &lt;a class="link" href="https://mirrors.tuna.tsinghua.edu.cn/help/docker-ce/" target="_blank" rel="noopener"
>清华大学开源软件镜像站 Docker Community Edition 镜像使用帮助&lt;/a>。&lt;/p>
&lt;h3 id="安装kubeadm">安装kubeadm&lt;/h3>
&lt;p>先信任软件仓库的证书，要注意的是证书托管在谷歌，所以基本不用考虑直接执行命令能成功了。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg &lt;span class="p">|&lt;/span> sudo apt-key add -
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>作为代替，可以先手动魔法上网下载到证书，再变通一下完成证书添加。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">cat apt-key.gpg &lt;span class="p">|&lt;/span> sudo apt-key add -
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>之后添加源，源的版本并不能直接对应到发行版的版本，目前 ubuntu server 只支持到 16.04 LTS ，或者 Debian 9 Stretch 。更高版本也可以装，但我比较怀疑官方的包到底有没有在新发行版里测试过，支持力度行不行。&lt;/p>
&lt;p>总之，如果宿主机不拿来当开发环境使的话，上个 Ubuntu server 16.04 LTS 也没事，只要还没有完全停止支持就好。总之这个问题上我保留意见吧。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="nb">echo&lt;/span> &lt;span class="s1">&amp;#39;deb https://mirrors.tuna.tsinghua.edu.cn/kubernetes/apt kubernetes-xenial main&amp;#39;&lt;/span> &lt;span class="p">|&lt;/span> sudo tee /etc/apt/sources.list.d/kubernetes.list
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>添加软件源之后更新软件包清单并安装 &lt;code>kubelet&lt;/code>、&lt;code>kubeadm&lt;/code>、&lt;code>kubectl&lt;/code> 。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="检查">检查&lt;/h3>
&lt;p>首先确认所有 &lt;code>kubelet&lt;/code>、&lt;code>kubeadm&lt;/code>、&lt;code>kubectl&lt;/code> 命令都已经可用，如果命令不存在则说明安装有问题，根据具体情况处理。&lt;/p>
&lt;p>然后检查&lt;code>kubelet&lt;/code>服务的状态（注意用了&lt;code>systemd&lt;/code>，不确定有没有用 &lt;code>upstart&lt;/code> 或别的 Unix 风格的服务管理的）。&lt;/p>
&lt;p>运行命令 &lt;code>sudo systemctl status kubelet&lt;/code> 得到下面的输出。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;span class="lnt">8
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-plaintext" data-lang="plaintext">● kubelet.service - kubelet: The Kubernetes Node Agent
Loaded: loaded (/lib/systemd/system/kubelet.service; enabled; vendor preset: enabled)
Drop-In: /etc/systemd/system/kubelet.service.d
└─10-kubeadm.conf
Active: activating (auto-restart) (Result: exit-code) since Fri 2021-11-19 02:32:29 UTC; 9s ago
Docs: https://kubernetes.io/docs/home/
Process: 6767 ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS (code=exited, status=1/FAILURE)
Main PID: 6767 (code=exited, status=1/FAILURE)
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>此时的 &lt;code>kubelet&lt;/code> 服务还是失败的状态，再检查 &lt;code>kubelet&lt;/code> 的日志，通过 &lt;code>sudo journalctl -u kubelet&lt;/code> 。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-plaintext" data-lang="plaintext">-- Logs begin at Thu 2021-11-18 07:40:58 UTC, end at Fri 2021-11-19 02:34:19 UTC. --
Nov 19 02:27:41 vm systemd[1]: Started kubelet: The Kubernetes Node Agent.
Nov 19 02:27:42 vm systemd[1]: kubelet.service: Current command vanished from the unit file, execution of the command list won&amp;#39;t be resumed.
Nov 19 02:27:42 vm systemd[1]: Stopping kubelet: The Kubernetes Node Agent...
Nov 19 02:27:42 vm systemd[1]: kubelet.service: Succeeded.
Nov 19 02:27:42 vm systemd[1]: Stopped kubelet: The Kubernetes Node Agent.
Nov 19 02:27:42 vm systemd[1]: Started kubelet: The Kubernetes Node Agent.
Nov 19 02:27:42 vm kubelet[5944]: E1119 02:27:42.559949 5944 server.go:206] &amp;#34;Failed to load kubelet config file&amp;#34; err=&amp;#34;failed to load Kubelet config file /var/lib/kubelet/config.yaml, error failed to read kube&amp;gt;
Nov 19 02:27:42 vm systemd[1]: kubelet.service: Main process exited, code=exited, status=1/FAILURE
Nov 19 02:27:42 vm systemd[1]: kubelet.service: Failed with result &amp;#39;exit-code&amp;#39;.
Nov 19 02:27:52 vm systemd[1]: kubelet.service: Scheduled restart job, restart counter is at 1.
Nov 19 02:27:52 vm systemd[1]: Stopped kubelet: The Kubernetes Node Agent.
Nov 19 02:27:52 vm systemd[1]: Started kubelet: The Kubernetes Node Agent.
Nov 19 02:27:52 vm kubelet[6119]: E1119 02:27:52.804723 6119 server.go:206] &amp;#34;Failed to load kubelet config file&amp;#34; err=&amp;#34;failed to load Kubelet config file /var/lib/kubelet/config.yaml, error failed to read kube&amp;gt;
Nov 19 02:27:52 vm systemd[1]: kubelet.service: Main process exited, code=exited, status=1/FAILURE
Nov 19 02:27:52 vm systemd[1]: kubelet.service: Failed with result &amp;#39;exit-code&amp;#39;.
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>失败的原因是 &lt;code>Failed to load kubelet config file&amp;quot; err=&amp;quot;failed to load Kubelet config file /var/lib/kubelet/config.yaml, error failed to read kube&amp;gt;&lt;/code>。&lt;/p>
&lt;h2 id="创建集群">创建集群&lt;/h2>
&lt;p>目标是创建一个单节点的集群。&lt;/p>
&lt;h3 id="拉取镜像">拉取镜像&lt;/h3>
&lt;p>众所周知的原因，&lt;code>kubernetes&lt;/code> 的镜像托管在谷歌服务器上，麻瓜是访问不到的，所以就连拉取镜像也值得用几十个字来说。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">sudo kubeadm config images pull --image-repository registry.cn-hangzhou.aliyuncs.com/google_containers
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="初始化主节点">初始化主节点&lt;/h3>
&lt;p>注意使用 &lt;code>kubeadm config images pull&lt;/code> 拉取了镜像的话，在 &lt;code>init&lt;/code> 阶段除非你把镜像 tag 给改了，不然也要传个 &lt;code>--image-repository&lt;/code> 参数。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">sudo kubeadm init --image-repository registry.cn-hangzhou.aliyuncs.com/google_containers
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>完整输出如下。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;span class="lnt">40
&lt;/span>&lt;span class="lnt">41
&lt;/span>&lt;span class="lnt">42
&lt;/span>&lt;span class="lnt">43
&lt;/span>&lt;span class="lnt">44
&lt;/span>&lt;span class="lnt">45
&lt;/span>&lt;span class="lnt">46
&lt;/span>&lt;span class="lnt">47
&lt;/span>&lt;span class="lnt">48
&lt;/span>&lt;span class="lnt">49
&lt;/span>&lt;span class="lnt">50
&lt;/span>&lt;span class="lnt">51
&lt;/span>&lt;span class="lnt">52
&lt;/span>&lt;span class="lnt">53
&lt;/span>&lt;span class="lnt">54
&lt;/span>&lt;span class="lnt">55
&lt;/span>&lt;span class="lnt">56
&lt;/span>&lt;span class="lnt">57
&lt;/span>&lt;span class="lnt">58
&lt;/span>&lt;span class="lnt">59
&lt;/span>&lt;span class="lnt">60
&lt;/span>&lt;span class="lnt">61
&lt;/span>&lt;span class="lnt">62
&lt;/span>&lt;span class="lnt">63
&lt;/span>&lt;span class="lnt">64
&lt;/span>&lt;span class="lnt">65
&lt;/span>&lt;span class="lnt">66
&lt;/span>&lt;span class="lnt">67
&lt;/span>&lt;span class="lnt">68
&lt;/span>&lt;span class="lnt">69
&lt;/span>&lt;span class="lnt">70
&lt;/span>&lt;span class="lnt">71
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-plaintext" data-lang="plaintext">[init] Using Kubernetes version: v1.22.4
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using &amp;#39;kubeadm config images pull&amp;#39;
[certs] Using certificateDir folder &amp;#34;/etc/kubernetes/pki&amp;#34;
[certs] Generating &amp;#34;ca&amp;#34; certificate and key
[certs] Generating &amp;#34;apiserver&amp;#34; certificate and key
[certs] apiserver serving cert is signed for DNS names [kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local vm] and IPs [10.96.0.1 10.0.2.15]
[certs] Generating &amp;#34;apiserver-kubelet-client&amp;#34; certificate and key
[certs] Generating &amp;#34;front-proxy-ca&amp;#34; certificate and key
[certs] Generating &amp;#34;front-proxy-client&amp;#34; certificate and key
[certs] Generating &amp;#34;etcd/ca&amp;#34; certificate and key
[certs] Generating &amp;#34;etcd/server&amp;#34; certificate and key
[certs] etcd/server serving cert is signed for DNS names [localhost vm] and IPs [10.0.2.15 127.0.0.1 ::1]
[certs] Generating &amp;#34;etcd/peer&amp;#34; certificate and key
[certs] etcd/peer serving cert is signed for DNS names [localhost vm] and IPs [10.0.2.15 127.0.0.1 ::1]
[certs] Generating &amp;#34;etcd/healthcheck-client&amp;#34; certificate and key
[certs] Generating &amp;#34;apiserver-etcd-client&amp;#34; certificate and key
[certs] Generating &amp;#34;sa&amp;#34; key and public key
[kubeconfig] Using kubeconfig folder &amp;#34;/etc/kubernetes&amp;#34;
[kubeconfig] Writing &amp;#34;admin.conf&amp;#34; kubeconfig file
[kubeconfig] Writing &amp;#34;kubelet.conf&amp;#34; kubeconfig file
[kubeconfig] Writing &amp;#34;controller-manager.conf&amp;#34; kubeconfig file
[kubeconfig] Writing &amp;#34;scheduler.conf&amp;#34; kubeconfig file
[kubelet-start] Writing kubelet environment file with flags to file &amp;#34;/var/lib/kubelet/kubeadm-flags.env&amp;#34;
[kubelet-start] Writing kubelet configuration to file &amp;#34;/var/lib/kubelet/config.yaml&amp;#34;
[kubelet-start] Starting the kubelet
[control-plane] Using manifest folder &amp;#34;/etc/kubernetes/manifests&amp;#34;
[control-plane] Creating static Pod manifest for &amp;#34;kube-apiserver&amp;#34;
[control-plane] Creating static Pod manifest for &amp;#34;kube-controller-manager&amp;#34;
[control-plane] Creating static Pod manifest for &amp;#34;kube-scheduler&amp;#34;
[etcd] Creating static Pod manifest for local etcd in &amp;#34;/etc/kubernetes/manifests&amp;#34;
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory &amp;#34;/etc/kubernetes/manifests&amp;#34;. This can take up to 4m0s
sudo kubeadm init --image-repository registry.cn-hangzhou.aliyuncs.com/google_containers[apiclient] All control plane components are healthy after 9.003038 seconds
[upload-config] Storing the configuration used in ConfigMap &amp;#34;kubeadm-config&amp;#34; in the &amp;#34;kube-system&amp;#34; Namespace
[kubelet] Creating a ConfigMap &amp;#34;kubelet-config-1.22&amp;#34; in namespace kube-system with the configuration for the kubelets in the cluster
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node vm as control-plane by adding the labels: [node-role.kubernetes.io/master(deprecated) node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node vm as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
[bootstrap-token] Using token: jfhacg.2ahc3yqndiwct9vk
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the &amp;#34;cluster-info&amp;#34; ConfigMap in the &amp;#34;kube-public&amp;#34; namespace
[kubelet-finalize] Updating &amp;#34;/etc/kubernetes/kubelet.conf&amp;#34; to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy
Your Kubernetes control-plane has initialized successfully!
To start using your cluster, you need to run the following as a regular user:
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
Alternatively, if you are the root user, you can run:
export KUBECONFIG=/etc/kubernetes/admin.conf
You should now deploy a pod network to the cluster.
Run &amp;#34;kubectl apply -f [podnetwork].yaml&amp;#34; with one of the options listed at:
https://kubernetes.io/docs/concepts/cluster-administration/addons/
Then you can join any number of worker nodes by running the following on each as root:
kubeadm join 10.0.2.15:6443 --token jfhacg.2ahc3yqndiwct9vk \
--discovery-token-ca-cert-hash sha256:377d6ead2bde8373000333d883c9bd9449233686fe277814ccade0b55fc362a1
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>因为是虚拟机里的集群，也没打算给任何人访问，关键信息懒得打码了。&lt;/p>
&lt;p>几个值得关注的内容：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="c1"># Your Kubernetes control-plane has initialized successfully!&lt;/span>
&lt;span class="c1"># To start using your cluster, you need to run the following as a regular user:&lt;/span>
mkdir -p &lt;span class="nv">$HOME&lt;/span>/.kube
sudo cp -i /etc/kubernetes/admin.conf &lt;span class="nv">$HOME&lt;/span>/.kube/config
sudo chown &lt;span class="k">$(&lt;/span>id -u&lt;span class="k">)&lt;/span>:&lt;span class="k">$(&lt;/span>id -g&lt;span class="k">)&lt;/span> &lt;span class="nv">$HOME&lt;/span>/.kube/config
&lt;span class="c1"># Alternatively, if you are the root user, you can run:&lt;/span>
&lt;span class="nb">export&lt;/span> &lt;span class="nv">KUBECONFIG&lt;/span>&lt;span class="o">=&lt;/span>/etc/kubernetes/admin.conf
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>首先，集群控制平面已经初始化成功了，说明命令执行基本 OK，没有致命错误。&lt;/p>
&lt;p>后面就是教你怎么配置 &lt;code>kubectl&lt;/code> 来访问控制平面，集群的管理员配置放在 &lt;code>/etc/kubernetes/admin.conf&lt;/code> ，可以用 &lt;code>KUBECONFIG&lt;/code> 环境变量来使用，或者把配置文件复制到家目录下的路径 &lt;code>~/.kube/config&lt;/code> 。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-plaintext" data-lang="plaintext">You should now deploy a pod network to the cluster.
Run &amp;#34;kubectl apply -f [podnetwork].yaml&amp;#34; with one of the options listed at:
https://kubernetes.io/docs/concepts/cluster-administration/addons/
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>提示你应该部署一个 POD 网络到集群，也就是一般说的 CNI 插件，以便 POD 之间可以互相通信。安装插件之前，集群的 DNS （&lt;code>CoreDNS&lt;/code>） 不会启动。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-plaintext" data-lang="plaintext">Then you can join any number of worker nodes by running the following on each as root:
kubeadm join 10.0.2.15:6443 --token jfhacg.2ahc3yqndiwct9vk \
--discovery-token-ca-cert-hash sha256:377d6ead2bde8373000333d883c9bd9449233686fe277814ccade0b55fc362a1
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>一旦搞定了网络插件，就可以用 &lt;code>kubeadm&lt;/code> 继续添加新的节点到集群里了。&lt;/p>
&lt;h3 id="安装网络插件">安装网络插件&lt;/h3>
&lt;p>看起来大家都在用 &lt;code>calico&lt;/code> 做 POD 网络，所以我也用 &lt;code>calico&lt;/code> 好了。步骤参考 &lt;code>calico&lt;/code> 的&lt;a class="link" href="https://docs.projectcalico.org/getting-started/kubernetes/self-managed-onprem/onpremises" target="_blank" rel="noopener"
>官方文档&lt;/a> 和 &lt;a class="link" href="https://docs.projectcalico.org/getting-started/kubernetes/quickstart" target="_blank" rel="noopener"
>官方的快速开始&lt;/a> 来配置一个单节点集群的 POD 。&lt;/p>
&lt;p>正式开始前，参考上面的内容配置好 &lt;code>kubectl&lt;/code> ，以便无需 root 权限运行 &lt;code>kubectl&lt;/code> 命令。&lt;/p>
&lt;p>先下载 &lt;code>calico&lt;/code> 的 k8s 资源。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">curl https://docs.projectcalico.org/manifests/calico-typha.yaml -o calico.yaml
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>按照说明，判断下 POD 的 CIDR（POD的网段），用 &lt;code>sudo kubectl --kubeconfig /etc/kubernetes/admin.conf get cm kubeadm-config -n kube-system -o yaml&lt;/code> 获取 &lt;code>kubeadm-config&lt;/code> 这个 &lt;code>configmap&lt;/code>，检查其中的 &lt;code>networking.podSubnet&lt;/code> 值。在我这里的输出如下。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-yaml" data-lang="yaml">&lt;span class="nt">apiVersion&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">v1&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w">&lt;/span>&lt;span class="nt">data&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">ClusterConfiguration&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">|&lt;/span>&lt;span class="sd">
&lt;/span>&lt;span class="sd"> apiServer:
&lt;/span>&lt;span class="sd"> extraArgs:
&lt;/span>&lt;span class="sd"> authorization-mode: Node,RBAC
&lt;/span>&lt;span class="sd"> timeoutForControlPlane: 4m0s
&lt;/span>&lt;span class="sd"> apiVersion: kubeadm.k8s.io/v1beta3
&lt;/span>&lt;span class="sd"> certificatesDir: /etc/kubernetes/pki
&lt;/span>&lt;span class="sd"> clusterName: kubernetes
&lt;/span>&lt;span class="sd"> controllerManager: {}
&lt;/span>&lt;span class="sd"> dns: {}
&lt;/span>&lt;span class="sd"> etcd:
&lt;/span>&lt;span class="sd"> local:
&lt;/span>&lt;span class="sd"> dataDir: /var/lib/etcd
&lt;/span>&lt;span class="sd"> imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers
&lt;/span>&lt;span class="sd"> kind: ClusterConfiguration
&lt;/span>&lt;span class="sd"> kubernetesVersion: v1.22.4
&lt;/span>&lt;span class="sd"> networking:
&lt;/span>&lt;span class="sd"> dnsDomain: cluster.local
&lt;/span>&lt;span class="sd"> serviceSubnet: 10.96.0.0/12
&lt;/span>&lt;span class="sd"> scheduler: {}&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w">&lt;/span>&lt;span class="nt">kind&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">ConfigMap&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w">&lt;/span>&lt;span class="nt">metadata&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">creationTimestamp&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s2">&amp;#34;2021-11-19T02:54:04Z&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">kubeadm-config&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">namespace&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">kube-system&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">resourceVersion&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s2">&amp;#34;210&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">uid&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">2567d366-2257-4114-8709-12b016cd1fe8&lt;/span>&lt;span class="w">
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>可以发现没有 &lt;code>podSubnet&lt;/code>，那就当是默认，按照 &lt;code>calico&lt;/code> 文档说明不用改 &lt;code>yaml&lt;/code>，正常应用。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">sudo kubectl --kubeconfig /etc/kubernetes/admin.conf apply -f calico.yaml
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>输出如下。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-plaintext" data-lang="plaintext">configmap/calico-config created
customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/caliconodestatuses.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipreservations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/kubecontrollersconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created
clusterrole.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrole.rbac.authorization.k8s.io/calico-node created
clusterrolebinding.rbac.authorization.k8s.io/calico-node created
service/calico-typha created
deployment.apps/calico-typha created
Warning: policy/v1beta1 PodDisruptionBudget is deprecated in v1.21+, unavailable in v1.25+; use policy/v1 PodDisruptionBudget
poddisruptionbudget.policy/calico-typha created
daemonset.apps/calico-node created
serviceaccount/calico-node created
deployment.apps/calico-kube-controllers created
serviceaccount/calico-kube-controllers created
poddisruptionbudget.policy/calico-kube-controllers created
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>出现了一个弃用警告，无视之，反正是 &lt;code>calico&lt;/code> 的问题。再检查下相关的 &lt;code>POD&lt;/code> 创建是否成功，用命令 &lt;code>kubectl get pod -n kube-system&lt;/code>，输出如下。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-plaintext" data-lang="plaintext">NAMESPACE NAME READY STATUS RESTARTS AGE
kube-system calico-kube-controllers-5d995d45d6-gwwrw 1/1 Running 0 5m29s
kube-system calico-node-sgb2x 0/1 Running 2 (49s ago) 5m29s
kube-system calico-typha-7df55cc78b-hpfkx 0/1 Pending 0 5m29s
kube-system coredns-7d89d9b6b8-c7sxl 1/1 Running 0 32m
kube-system coredns-7d89d9b6b8-tjsj8 1/1 Running 0 32m
kube-system etcd-vm 1/1 Running 0 32m
kube-system kube-apiserver-vm 1/1 Running 0 32m
kube-system kube-controller-manager-vm 1/1 Running 0 32m
kube-system kube-proxy-d64kh 1/1 Running 0 32m
kube-system kube-scheduler-vm 1/1 Running 0 32m
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>可以看到至少镜像是拉到了。&lt;/p>
&lt;p>&lt;code>calico-typha-7df55cc78b-hpfkx&lt;/code> 这个 POD 的 &lt;code>describe&lt;/code> 显示不能运行在 &lt;code>master&lt;/code> 节点。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-plaintext" data-lang="plaintext">Events:
Type Reason Age From Message
---- ------ ---- ---- -------
Warning FailedScheduling 56s (x9 over 8m57s) default-scheduler 0/1 nodes are available: 1 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn&amp;#39;t tolerate.
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>而 &lt;code>calico-node-sgb2x&lt;/code> 的日志显示需要 &lt;code>calico-typha&lt;/code> 才能运行。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-plaintext" data-lang="plaintext">2021-11-19 03:27:46.413 [ERROR][1674] confd/discovery.go 153: Didn&amp;#39;t find any ready Typha instances.
2021-11-19 03:27:46.413 [FATAL][1674] confd/startsyncerclient.go 48: Typha discovery enabled but discovery failed. error=Kubernetes service missing IP or port
bird: Unable to open configuration file /etc/calico/confd/config/bird6.cfg: No such file or directory
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>因为想要的是一个单节点集群，所以接下来把本节点的污点 &lt;code>node-role.kubernetes.io/master-&lt;/code> 给去掉。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">sudo kubectl --kubeconfig /etc/kubernetes/admin.conf taint nodes --all node-role.kubernetes.io/master-
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>输出&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-plaintext" data-lang="plaintext">node/vm untainted
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>再观察 &lt;code>kube-system&lt;/code> 里的 POD 状态。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-plaintext" data-lang="plaintext">NAME READY STATUS RESTARTS AGE
calico-kube-controllers-5d995d45d6-gwwrw 1/1 Running 0 11m
calico-node-sgb2x 1/1 Running 7 (38s ago) 11m
calico-typha-7df55cc78b-hpfkx 1/1 Running 0 11m
coredns-7d89d9b6b8-c7sxl 1/1 Running 0 38m
coredns-7d89d9b6b8-tjsj8 1/1 Running 0 38m
etcd-vm 1/1 Running 0 38m
kube-apiserver-vm 1/1 Running 0 38m
kube-controller-manager-vm 1/1 Running 0 38m
kube-proxy-d64kh 1/1 Running 0 38m
kube-scheduler-vm 1/1 Running 0 38m
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>可以看到所有的POD都已经进入&lt;code>Ready&lt;/code>状态。&lt;/p>
&lt;p>最后通过 &lt;code>kubectl get nodes -o wide&lt;/code> 检查节点状态。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-plaintext" data-lang="plaintext">NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME
vm Ready control-plane,master 39m v1.22.4 10.0.2.15 &amp;lt;none&amp;gt; Ubuntu 20.04.3 LTS 5.4.0-90-generic docker://20.10.11
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>到这里单节点集群就成功部署了。&lt;/p>
&lt;h2 id="总结">总结&lt;/h2>
&lt;p>之后还可以部署 dashboard 之类的应用验证，不想写了，浪费时间。&lt;/p>
&lt;p>写了一大堆又删掉了。&lt;/p>
&lt;p>如果一定要总结的话，k8s，学了进小厂吧，小厂不用；学了进大厂吧，大厂也不要你。&lt;/p></description></item><item><title>slackware 和虚拟机基本配置</title><link>https://nnnewb.github.io/blog/p/slackware-%E5%92%8C%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%9F%BA%E6%9C%AC%E9%85%8D%E7%BD%AE/</link><pubDate>Wed, 30 Dec 2020 11:11:56 +0800</pubDate><guid>https://nnnewb.github.io/blog/p/slackware-%E5%92%8C%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%9F%BA%E6%9C%AC%E9%85%8D%E7%BD%AE/</guid><description>&lt;p>slackware 是一个非常有极客味的 Linux 发行版，因为官方维护的包不多，基本靠 slackbuilds 续命。&lt;/p>
&lt;p>slackware 的一个特色是包管理系统不处理依赖关系，这一点劝退不少人。&lt;/p>
&lt;p>实际上，虽然我不是很赞同 &lt;a class="link" href="https://docs.slackware.com/start?id=slackware:package_and_dependency_management_shouldn_t_put_you_off_slackware" target="_blank" rel="noopener"
>这个观点&lt;/a> ，不过并不妨碍 slackware 成为可玩性相对高的 Linux 发行版之一（另外几个可玩性不错的发行版包括 Arch Linux 和 Gentoo）。&lt;/p>
&lt;p>这篇博文实际上就是安利下 slackware 并且简要介绍下怎么在虚拟机里搭建个基本环境来体验游玩。&lt;/p>
&lt;!-- more -->
&lt;h2 id="0x01-安装">0x01 安装&lt;/h2>
&lt;p>安装的参考文档太多了，个人认为主要的难点在分区和引导。毕竟不像其他更流行的发行版的 GUI 安装引导，对 fdisk 和 parted 这些工具不熟悉、对操作系统引导启动的一些基本概念、原理不了解的人很容易犯下错误而不自知。&lt;/p>
&lt;p>这里提供一篇之前在贴吧写的 &lt;a class="link" href="https://tieba.baidu.com/p/4863103375" target="_blank" rel="noopener"
>安装教程&lt;/a> ，不做赘述了。&lt;/p>
&lt;h2 id="0x02-桌面">0x02 桌面&lt;/h2>
&lt;p>对习惯了装完就有桌面的用户来说，安装完 slackware 之后遇到的第一个问题就是怎么进入桌面——甚至会问怎么登陆。&lt;/p>
&lt;p>这里就挂一张 gif 好了。&lt;/p>
&lt;p>&lt;img src="https://nnnewb.github.io/blog/blog/image/slackware-vm-setup/01.gif"
loading="lazy"
alt="01"
>&lt;/p>
&lt;p>假设没手贱在安装的时候把 x/kde/xfce 之类的软件包组给去掉的话，就不会有什么问题。&lt;/p>
&lt;p>如果需要自动进入桌面，需要手动修改 &lt;code>/etc/inittab&lt;/code> 文件，把默认的 runlevel 修改为 4 。&lt;/p>
&lt;p>具体怎么改，看 gif 。&lt;/p>
&lt;p>&lt;img src="https://nnnewb.github.io/blog/blog/image/slackware-vm-setup/02.gif"
loading="lazy"
alt="02"
>&lt;/p>
&lt;h2 id="0x03-slackpkg-包管理">0x03 slackpkg 包管理&lt;/h2>
&lt;p>如果用过 ubuntu ，那么下一个问题可能就是 &amp;ldquo;怎么没有 apt-get 命令？&amp;rdquo; 或者 &amp;ldquo;slackware 用什么命令安装软件？&amp;rdquo;&lt;/p>
&lt;p>答案是有好几个相关命令。&lt;/p>
&lt;ul>
&lt;li>installpkg&lt;/li>
&lt;li>removepkg&lt;/li>
&lt;li>upgradepkg&lt;/li>
&lt;li>makepkg&lt;/li>
&lt;li>explodepkg&lt;/li>
&lt;li>rpm2targz&lt;/li>
&lt;/ul>
&lt;p>大部分命令顾名思义，也不需要额外说明。如果说和 apt 或者 pacman 类似的一个统一的包管理器的话，那就是 slackpkg 。&lt;/p>
&lt;p>使用 slackpkg 之前，需要手动修改 /etc/slackpkg/mirrors 文件，选择一个网络状况比较好的软件源地址，把行开头的 # 号去掉。&lt;/p>
&lt;p>完事之后用命令 &lt;code>slackpkg update&lt;/code> 更新一下本地索引，就可以正常用了。&lt;/p>
&lt;p>常用的命令包括&lt;/p>
&lt;ul>
&lt;li>slackpkg search&lt;/li>
&lt;li>slackpkg file-search&lt;/li>
&lt;li>slackpkg install&lt;/li>
&lt;li>slackpkg install-new&lt;/li>
&lt;li>slackpkg upgrade&lt;/li>
&lt;li>slackpkg upgrade-all&lt;/li>
&lt;/ul>
&lt;p>具体不细说了，看参考链接，或者自己看看 &lt;code>man slackpkg&lt;/code> 或者 &lt;code>slackpkg help&lt;/code>&lt;/p>
&lt;p>此外还有个不常用的，和安装时的 &lt;code>setup&lt;/code> 风格比较类似的工具，&lt;code>pkgtool&lt;/code>。具体可以自己看看命令。&lt;/p>
&lt;h2 id="0x04-slackbuilds">0x04 SlackBuilds&lt;/h2>
&lt;p>用过 Arch Linux 的 AUR 的用户对这种第三方维护的软件包会比较熟悉， SlackBuilds 对这些用户来说就是另一个 AUR 而已。&lt;/p>
&lt;p>不同之处在于，SlackBuilds 需要手动下载脚本和源码，然后自己看 README 再运行编译。&lt;/p>
&lt;p>当然这不是说 SlackBuilds 没有类似 yaourt 或者 yay 之类的自动工具，你可以试试 sbopkg 。&lt;/p>
&lt;p>这里给个简单的例子，用 sbopkg 安装 fbterm 。&lt;/p>
&lt;p>&lt;img src="https://nnnewb.github.io/blog/blog/image/slackware-vm-setup/03.gif"
loading="lazy"
alt="03.gif"
>&lt;/p>
&lt;h2 id="0x05-编写-slackbuilds">0x05 编写 SlackBuilds&lt;/h2>
&lt;p>讲道理，slackware 常用的软件太少，基本全靠 slackbuilds 撑场面。如果 SlackBuilds 上也没有呢？&lt;/p>
&lt;p>那只能自己写吧。&lt;/p>
&lt;p>对于熟悉 bash 脚本的用户来说这不是什么难事。这篇 &lt;a class="link" href="https://slackwiki.com/Writing_A_SlackBuild_Script" target="_blank" rel="noopener"
>HOWTO 文章&lt;/a> 很好地说明了怎么写一个 SlackBuilds 脚本。&lt;/p>
&lt;h2 id="0x06-参与社区">0x06 参与社区&lt;/h2>
&lt;p>slackware 中文社区太小了，或者说根本不存在。&lt;/p>
&lt;p>能聊几句的基本只有贴吧（实际上现在也找不到人了）或者 GitHub 上（slackwarecn 社区也不活跃）。&lt;/p>
&lt;p>如果对 slackware 感兴趣，可以玩一玩，写几个常用软件的 SlackBuilds 脚本什么的。&lt;/p>
&lt;p>就这样吧。&lt;/p></description></item></channel></rss>