<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>linux on weakptr's 笔记</title><link>https://nnnewb.github.io/blog/tags/linux/</link><description>Recent content in linux on weakptr's 笔记</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><lastBuildDate>Thu, 25 Nov 2021 14:31:00 +0800</lastBuildDate><atom:link href="https://nnnewb.github.io/blog/tags/linux/index.xml" rel="self" type="application/rss+xml"/><item><title>kubeadm安装实验集群记录</title><link>https://nnnewb.github.io/blog/p/kubernetes-manually-install-by-kubeadm/</link><pubDate>Thu, 25 Nov 2021 14:31:00 +0800</pubDate><guid>https://nnnewb.github.io/blog/p/kubernetes-manually-install-by-kubeadm/</guid><description>&lt;h2 id="前言">前言&lt;/h2>
&lt;p>好吧，如果仔细想想就会发现不管是 k3s 还是 ucloud 上的 k8s ，都没有一个是自己手动配置好的。虽说并不是至关重要的，但手动用 kubeadm 装一次 kubernetes 总不会有什么坏处。顺手做个笔记。参考资料列出如下。&lt;/p>
&lt;ul>
&lt;li>&lt;a class="link" href="https://kubernetes.io/zh/docs/setup/production-environment/tools/kubeadm/install-kubeadm/" target="_blank" rel="noopener"
>https://kubernetes.io/zh/docs/setup/production-environment/tools/kubeadm/install-kubeadm/&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://mirrors.tuna.tsinghua.edu.cn/help/kubernetes/" target="_blank" rel="noopener"
>https://mirrors.tuna.tsinghua.edu.cn/help/kubernetes/&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://computingforgeeks.com/deploy-kubernetes-cluster-on-ubuntu-with-kubeadm/" target="_blank" rel="noopener"
>https://computingforgeeks.com/deploy-kubernetes-cluster-on-ubuntu-with-kubeadm/&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="系统配置">系统配置&lt;/h2>
&lt;p>正式安装之前先确认一些系统级配置。&lt;/p>
&lt;h3 id="swapoff">swapoff&lt;/h3>
&lt;p>简单的做法是 &lt;code>sudo swapoff -a&lt;/code> 即可。之后改 &lt;code>fstab&lt;/code> 把 &lt;code>swap&lt;/code> 分区关掉。&lt;/p>
&lt;h3 id="iptables检查桥接流量">iptables检查桥接流量&lt;/h3>
&lt;p>用 &lt;code>lsmod | grep bf_netfitler&lt;/code> 检查有没有启用 &lt;code>bf_netfilter&lt;/code> 模块，如果没有输出的话说明没加载，执行下面的命令。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">cat &lt;span class="s">&amp;lt;&amp;lt;EOF | sudo tee /etc/modules-load.d/k8s.conf
&lt;/span>&lt;span class="s">br_netfilter
&lt;/span>&lt;span class="s">EOF&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>会在 &lt;code>/etc/modules-load.d&lt;/code> 下添加一个模块自动加载的配置。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">cat &lt;span class="s">&amp;lt;&amp;lt;EOF | sudo tee /etc/sysctl.d/k8s.conf
&lt;/span>&lt;span class="s">net.bridge.bridge-nf-call-ip6tables = 1
&lt;/span>&lt;span class="s">net.bridge.bridge-nf-call-iptables = 1
&lt;/span>&lt;span class="s">EOF&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>再在 &lt;code>/etc/sysctl.d/&lt;/code> 下添加一个配置，允许 &lt;code>iptables&lt;/code> 查看桥接流量。&lt;/p>
&lt;p>然后用 &lt;code>sysctl&lt;/code> 重载配置。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">sudo sysctl --system
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="端口">端口&lt;/h3>
&lt;p>控制平面节点的端口清单，如果有本机防火墙的话需要开放下面的端口。&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>协议&lt;/th>
&lt;th>方向&lt;/th>
&lt;th>端口范围&lt;/th>
&lt;th>作用&lt;/th>
&lt;th>使用者&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>TCP&lt;/td>
&lt;td>入站&lt;/td>
&lt;td>6443&lt;/td>
&lt;td>Kubernetes API 服务器&lt;/td>
&lt;td>所有组件&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>TCP&lt;/td>
&lt;td>入站&lt;/td>
&lt;td>2379-2380&lt;/td>
&lt;td>etcd 服务器客户端 API&lt;/td>
&lt;td>kube-apiserver, etcd&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>TCP&lt;/td>
&lt;td>入站&lt;/td>
&lt;td>10250&lt;/td>
&lt;td>Kubelet API&lt;/td>
&lt;td>kubelet 自身、控制平面组件&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>TCP&lt;/td>
&lt;td>入站&lt;/td>
&lt;td>10251&lt;/td>
&lt;td>kube-scheduler&lt;/td>
&lt;td>kube-scheduler 自身&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>TCP&lt;/td>
&lt;td>入站&lt;/td>
&lt;td>10252&lt;/td>
&lt;td>kube-controller-manager&lt;/td>
&lt;td>kube-controller-manager 自身&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>工作节点的端口清单，如果有本机防火墙的话需要开放下面的端口。&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>协议&lt;/th>
&lt;th>方向&lt;/th>
&lt;th>端口范围&lt;/th>
&lt;th>作用&lt;/th>
&lt;th>使用者&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>TCP&lt;/td>
&lt;td>入站&lt;/td>
&lt;td>10250&lt;/td>
&lt;td>Kubelet API&lt;/td>
&lt;td>kubelet 自身、控制平面组件&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>TCP&lt;/td>
&lt;td>入站&lt;/td>
&lt;td>30000-32767&lt;/td>
&lt;td>NodePort 服务†&lt;/td>
&lt;td>所有组件&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="容器运行时">容器运行时&lt;/h3>
&lt;p>参考 &lt;a class="link" href="https://mirrors.tuna.tsinghua.edu.cn/help/docker-ce/" target="_blank" rel="noopener"
>清华大学开源软件镜像站 Docker Community Edition 镜像使用帮助&lt;/a>。&lt;/p>
&lt;h3 id="安装kubeadm">安装kubeadm&lt;/h3>
&lt;p>先信任软件仓库的证书，要注意的是证书托管在谷歌，所以基本不用考虑直接执行命令能成功了。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg &lt;span class="p">|&lt;/span> sudo apt-key add -
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>作为代替，可以先手动魔法上网下载到证书，再变通一下完成证书添加。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">cat apt-key.gpg &lt;span class="p">|&lt;/span> sudo apt-key add -
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>之后添加源，源的版本并不能直接对应到发行版的版本，目前 ubuntu server 只支持到 16.04 LTS ，或者 Debian 9 Stretch 。更高版本也可以装，但我比较怀疑官方的包到底有没有在新发行版里测试过，支持力度行不行。&lt;/p>
&lt;p>总之，如果宿主机不拿来当开发环境使的话，上个 Ubuntu server 16.04 LTS 也没事，只要还没有完全停止支持就好。总之这个问题上我保留意见吧。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="nb">echo&lt;/span> &lt;span class="s1">&amp;#39;deb https://mirrors.tuna.tsinghua.edu.cn/kubernetes/apt kubernetes-xenial main&amp;#39;&lt;/span> &lt;span class="p">|&lt;/span> sudo tee /etc/apt/sources.list.d/kubernetes.list
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>添加软件源之后更新软件包清单并安装 &lt;code>kubelet&lt;/code>、&lt;code>kubeadm&lt;/code>、&lt;code>kubectl&lt;/code> 。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="检查">检查&lt;/h3>
&lt;p>首先确认所有 &lt;code>kubelet&lt;/code>、&lt;code>kubeadm&lt;/code>、&lt;code>kubectl&lt;/code> 命令都已经可用，如果命令不存在则说明安装有问题，根据具体情况处理。&lt;/p>
&lt;p>然后检查&lt;code>kubelet&lt;/code>服务的状态（注意用了&lt;code>systemd&lt;/code>，不确定有没有用 &lt;code>upstart&lt;/code> 或别的 Unix 风格的服务管理的）。&lt;/p>
&lt;p>运行命令 &lt;code>sudo systemctl status kubelet&lt;/code> 得到下面的输出。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;span class="lnt">8
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-plaintext" data-lang="plaintext">● kubelet.service - kubelet: The Kubernetes Node Agent
Loaded: loaded (/lib/systemd/system/kubelet.service; enabled; vendor preset: enabled)
Drop-In: /etc/systemd/system/kubelet.service.d
└─10-kubeadm.conf
Active: activating (auto-restart) (Result: exit-code) since Fri 2021-11-19 02:32:29 UTC; 9s ago
Docs: https://kubernetes.io/docs/home/
Process: 6767 ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS (code=exited, status=1/FAILURE)
Main PID: 6767 (code=exited, status=1/FAILURE)
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>此时的 &lt;code>kubelet&lt;/code> 服务还是失败的状态，再检查 &lt;code>kubelet&lt;/code> 的日志，通过 &lt;code>sudo journalctl -u kubelet&lt;/code> 。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-plaintext" data-lang="plaintext">-- Logs begin at Thu 2021-11-18 07:40:58 UTC, end at Fri 2021-11-19 02:34:19 UTC. --
Nov 19 02:27:41 vm systemd[1]: Started kubelet: The Kubernetes Node Agent.
Nov 19 02:27:42 vm systemd[1]: kubelet.service: Current command vanished from the unit file, execution of the command list won&amp;#39;t be resumed.
Nov 19 02:27:42 vm systemd[1]: Stopping kubelet: The Kubernetes Node Agent...
Nov 19 02:27:42 vm systemd[1]: kubelet.service: Succeeded.
Nov 19 02:27:42 vm systemd[1]: Stopped kubelet: The Kubernetes Node Agent.
Nov 19 02:27:42 vm systemd[1]: Started kubelet: The Kubernetes Node Agent.
Nov 19 02:27:42 vm kubelet[5944]: E1119 02:27:42.559949 5944 server.go:206] &amp;#34;Failed to load kubelet config file&amp;#34; err=&amp;#34;failed to load Kubelet config file /var/lib/kubelet/config.yaml, error failed to read kube&amp;gt;
Nov 19 02:27:42 vm systemd[1]: kubelet.service: Main process exited, code=exited, status=1/FAILURE
Nov 19 02:27:42 vm systemd[1]: kubelet.service: Failed with result &amp;#39;exit-code&amp;#39;.
Nov 19 02:27:52 vm systemd[1]: kubelet.service: Scheduled restart job, restart counter is at 1.
Nov 19 02:27:52 vm systemd[1]: Stopped kubelet: The Kubernetes Node Agent.
Nov 19 02:27:52 vm systemd[1]: Started kubelet: The Kubernetes Node Agent.
Nov 19 02:27:52 vm kubelet[6119]: E1119 02:27:52.804723 6119 server.go:206] &amp;#34;Failed to load kubelet config file&amp;#34; err=&amp;#34;failed to load Kubelet config file /var/lib/kubelet/config.yaml, error failed to read kube&amp;gt;
Nov 19 02:27:52 vm systemd[1]: kubelet.service: Main process exited, code=exited, status=1/FAILURE
Nov 19 02:27:52 vm systemd[1]: kubelet.service: Failed with result &amp;#39;exit-code&amp;#39;.
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>失败的原因是 &lt;code>Failed to load kubelet config file&amp;quot; err=&amp;quot;failed to load Kubelet config file /var/lib/kubelet/config.yaml, error failed to read kube&amp;gt;&lt;/code>。&lt;/p>
&lt;h2 id="创建集群">创建集群&lt;/h2>
&lt;p>目标是创建一个单节点的集群。&lt;/p>
&lt;h3 id="拉取镜像">拉取镜像&lt;/h3>
&lt;p>众所周知的原因，&lt;code>kubernetes&lt;/code> 的镜像托管在谷歌服务器上，麻瓜是访问不到的，所以就连拉取镜像也值得用几十个字来说。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">sudo kubeadm config images pull --image-repository registry.cn-hangzhou.aliyuncs.com/google_containers
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="初始化主节点">初始化主节点&lt;/h3>
&lt;p>注意使用 &lt;code>kubeadm config images pull&lt;/code> 拉取了镜像的话，在 &lt;code>init&lt;/code> 阶段除非你把镜像 tag 给改了，不然也要传个 &lt;code>--image-repository&lt;/code> 参数。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">sudo kubeadm init --image-repository registry.cn-hangzhou.aliyuncs.com/google_containers
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>完整输出如下。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;span class="lnt">40
&lt;/span>&lt;span class="lnt">41
&lt;/span>&lt;span class="lnt">42
&lt;/span>&lt;span class="lnt">43
&lt;/span>&lt;span class="lnt">44
&lt;/span>&lt;span class="lnt">45
&lt;/span>&lt;span class="lnt">46
&lt;/span>&lt;span class="lnt">47
&lt;/span>&lt;span class="lnt">48
&lt;/span>&lt;span class="lnt">49
&lt;/span>&lt;span class="lnt">50
&lt;/span>&lt;span class="lnt">51
&lt;/span>&lt;span class="lnt">52
&lt;/span>&lt;span class="lnt">53
&lt;/span>&lt;span class="lnt">54
&lt;/span>&lt;span class="lnt">55
&lt;/span>&lt;span class="lnt">56
&lt;/span>&lt;span class="lnt">57
&lt;/span>&lt;span class="lnt">58
&lt;/span>&lt;span class="lnt">59
&lt;/span>&lt;span class="lnt">60
&lt;/span>&lt;span class="lnt">61
&lt;/span>&lt;span class="lnt">62
&lt;/span>&lt;span class="lnt">63
&lt;/span>&lt;span class="lnt">64
&lt;/span>&lt;span class="lnt">65
&lt;/span>&lt;span class="lnt">66
&lt;/span>&lt;span class="lnt">67
&lt;/span>&lt;span class="lnt">68
&lt;/span>&lt;span class="lnt">69
&lt;/span>&lt;span class="lnt">70
&lt;/span>&lt;span class="lnt">71
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-plaintext" data-lang="plaintext">[init] Using Kubernetes version: v1.22.4
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using &amp;#39;kubeadm config images pull&amp;#39;
[certs] Using certificateDir folder &amp;#34;/etc/kubernetes/pki&amp;#34;
[certs] Generating &amp;#34;ca&amp;#34; certificate and key
[certs] Generating &amp;#34;apiserver&amp;#34; certificate and key
[certs] apiserver serving cert is signed for DNS names [kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local vm] and IPs [10.96.0.1 10.0.2.15]
[certs] Generating &amp;#34;apiserver-kubelet-client&amp;#34; certificate and key
[certs] Generating &amp;#34;front-proxy-ca&amp;#34; certificate and key
[certs] Generating &amp;#34;front-proxy-client&amp;#34; certificate and key
[certs] Generating &amp;#34;etcd/ca&amp;#34; certificate and key
[certs] Generating &amp;#34;etcd/server&amp;#34; certificate and key
[certs] etcd/server serving cert is signed for DNS names [localhost vm] and IPs [10.0.2.15 127.0.0.1 ::1]
[certs] Generating &amp;#34;etcd/peer&amp;#34; certificate and key
[certs] etcd/peer serving cert is signed for DNS names [localhost vm] and IPs [10.0.2.15 127.0.0.1 ::1]
[certs] Generating &amp;#34;etcd/healthcheck-client&amp;#34; certificate and key
[certs] Generating &amp;#34;apiserver-etcd-client&amp;#34; certificate and key
[certs] Generating &amp;#34;sa&amp;#34; key and public key
[kubeconfig] Using kubeconfig folder &amp;#34;/etc/kubernetes&amp;#34;
[kubeconfig] Writing &amp;#34;admin.conf&amp;#34; kubeconfig file
[kubeconfig] Writing &amp;#34;kubelet.conf&amp;#34; kubeconfig file
[kubeconfig] Writing &amp;#34;controller-manager.conf&amp;#34; kubeconfig file
[kubeconfig] Writing &amp;#34;scheduler.conf&amp;#34; kubeconfig file
[kubelet-start] Writing kubelet environment file with flags to file &amp;#34;/var/lib/kubelet/kubeadm-flags.env&amp;#34;
[kubelet-start] Writing kubelet configuration to file &amp;#34;/var/lib/kubelet/config.yaml&amp;#34;
[kubelet-start] Starting the kubelet
[control-plane] Using manifest folder &amp;#34;/etc/kubernetes/manifests&amp;#34;
[control-plane] Creating static Pod manifest for &amp;#34;kube-apiserver&amp;#34;
[control-plane] Creating static Pod manifest for &amp;#34;kube-controller-manager&amp;#34;
[control-plane] Creating static Pod manifest for &amp;#34;kube-scheduler&amp;#34;
[etcd] Creating static Pod manifest for local etcd in &amp;#34;/etc/kubernetes/manifests&amp;#34;
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory &amp;#34;/etc/kubernetes/manifests&amp;#34;. This can take up to 4m0s
sudo kubeadm init --image-repository registry.cn-hangzhou.aliyuncs.com/google_containers[apiclient] All control plane components are healthy after 9.003038 seconds
[upload-config] Storing the configuration used in ConfigMap &amp;#34;kubeadm-config&amp;#34; in the &amp;#34;kube-system&amp;#34; Namespace
[kubelet] Creating a ConfigMap &amp;#34;kubelet-config-1.22&amp;#34; in namespace kube-system with the configuration for the kubelets in the cluster
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node vm as control-plane by adding the labels: [node-role.kubernetes.io/master(deprecated) node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node vm as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
[bootstrap-token] Using token: jfhacg.2ahc3yqndiwct9vk
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the &amp;#34;cluster-info&amp;#34; ConfigMap in the &amp;#34;kube-public&amp;#34; namespace
[kubelet-finalize] Updating &amp;#34;/etc/kubernetes/kubelet.conf&amp;#34; to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy
Your Kubernetes control-plane has initialized successfully!
To start using your cluster, you need to run the following as a regular user:
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
Alternatively, if you are the root user, you can run:
export KUBECONFIG=/etc/kubernetes/admin.conf
You should now deploy a pod network to the cluster.
Run &amp;#34;kubectl apply -f [podnetwork].yaml&amp;#34; with one of the options listed at:
https://kubernetes.io/docs/concepts/cluster-administration/addons/
Then you can join any number of worker nodes by running the following on each as root:
kubeadm join 10.0.2.15:6443 --token jfhacg.2ahc3yqndiwct9vk \
--discovery-token-ca-cert-hash sha256:377d6ead2bde8373000333d883c9bd9449233686fe277814ccade0b55fc362a1
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>因为是虚拟机里的集群，也没打算给任何人访问，关键信息懒得打码了。&lt;/p>
&lt;p>几个值得关注的内容：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="c1"># Your Kubernetes control-plane has initialized successfully!&lt;/span>
&lt;span class="c1"># To start using your cluster, you need to run the following as a regular user:&lt;/span>
mkdir -p &lt;span class="nv">$HOME&lt;/span>/.kube
sudo cp -i /etc/kubernetes/admin.conf &lt;span class="nv">$HOME&lt;/span>/.kube/config
sudo chown &lt;span class="k">$(&lt;/span>id -u&lt;span class="k">)&lt;/span>:&lt;span class="k">$(&lt;/span>id -g&lt;span class="k">)&lt;/span> &lt;span class="nv">$HOME&lt;/span>/.kube/config
&lt;span class="c1"># Alternatively, if you are the root user, you can run:&lt;/span>
&lt;span class="nb">export&lt;/span> &lt;span class="nv">KUBECONFIG&lt;/span>&lt;span class="o">=&lt;/span>/etc/kubernetes/admin.conf
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>首先，集群控制平面已经初始化成功了，说明命令执行基本 OK，没有致命错误。&lt;/p>
&lt;p>后面就是教你怎么配置 &lt;code>kubectl&lt;/code> 来访问控制平面，集群的管理员配置放在 &lt;code>/etc/kubernetes/admin.conf&lt;/code> ，可以用 &lt;code>KUBECONFIG&lt;/code> 环境变量来使用，或者把配置文件复制到家目录下的路径 &lt;code>~/.kube/config&lt;/code> 。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-plaintext" data-lang="plaintext">You should now deploy a pod network to the cluster.
Run &amp;#34;kubectl apply -f [podnetwork].yaml&amp;#34; with one of the options listed at:
https://kubernetes.io/docs/concepts/cluster-administration/addons/
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>提示你应该部署一个 POD 网络到集群，也就是一般说的 CNI 插件，以便 POD 之间可以互相通信。安装插件之前，集群的 DNS （&lt;code>CoreDNS&lt;/code>） 不会启动。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-plaintext" data-lang="plaintext">Then you can join any number of worker nodes by running the following on each as root:
kubeadm join 10.0.2.15:6443 --token jfhacg.2ahc3yqndiwct9vk \
--discovery-token-ca-cert-hash sha256:377d6ead2bde8373000333d883c9bd9449233686fe277814ccade0b55fc362a1
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>一旦搞定了网络插件，就可以用 &lt;code>kubeadm&lt;/code> 继续添加新的节点到集群里了。&lt;/p>
&lt;h3 id="安装网络插件">安装网络插件&lt;/h3>
&lt;p>看起来大家都在用 &lt;code>calico&lt;/code> 做 POD 网络，所以我也用 &lt;code>calico&lt;/code> 好了。步骤参考 &lt;code>calico&lt;/code> 的&lt;a class="link" href="https://docs.projectcalico.org/getting-started/kubernetes/self-managed-onprem/onpremises" target="_blank" rel="noopener"
>官方文档&lt;/a> 和 &lt;a class="link" href="https://docs.projectcalico.org/getting-started/kubernetes/quickstart" target="_blank" rel="noopener"
>官方的快速开始&lt;/a> 来配置一个单节点集群的 POD 。&lt;/p>
&lt;p>正式开始前，参考上面的内容配置好 &lt;code>kubectl&lt;/code> ，以便无需 root 权限运行 &lt;code>kubectl&lt;/code> 命令。&lt;/p>
&lt;p>先下载 &lt;code>calico&lt;/code> 的 k8s 资源。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">curl https://docs.projectcalico.org/manifests/calico-typha.yaml -o calico.yaml
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>按照说明，判断下 POD 的 CIDR（POD的网段），用 &lt;code>sudo kubectl --kubeconfig /etc/kubernetes/admin.conf get cm kubeadm-config -n kube-system -o yaml&lt;/code> 获取 &lt;code>kubeadm-config&lt;/code> 这个 &lt;code>configmap&lt;/code>，检查其中的 &lt;code>networking.podSubnet&lt;/code> 值。在我这里的输出如下。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-yaml" data-lang="yaml">&lt;span class="nt">apiVersion&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">v1&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w">&lt;/span>&lt;span class="nt">data&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">ClusterConfiguration&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">|&lt;/span>&lt;span class="sd">
&lt;/span>&lt;span class="sd"> apiServer:
&lt;/span>&lt;span class="sd"> extraArgs:
&lt;/span>&lt;span class="sd"> authorization-mode: Node,RBAC
&lt;/span>&lt;span class="sd"> timeoutForControlPlane: 4m0s
&lt;/span>&lt;span class="sd"> apiVersion: kubeadm.k8s.io/v1beta3
&lt;/span>&lt;span class="sd"> certificatesDir: /etc/kubernetes/pki
&lt;/span>&lt;span class="sd"> clusterName: kubernetes
&lt;/span>&lt;span class="sd"> controllerManager: {}
&lt;/span>&lt;span class="sd"> dns: {}
&lt;/span>&lt;span class="sd"> etcd:
&lt;/span>&lt;span class="sd"> local:
&lt;/span>&lt;span class="sd"> dataDir: /var/lib/etcd
&lt;/span>&lt;span class="sd"> imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers
&lt;/span>&lt;span class="sd"> kind: ClusterConfiguration
&lt;/span>&lt;span class="sd"> kubernetesVersion: v1.22.4
&lt;/span>&lt;span class="sd"> networking:
&lt;/span>&lt;span class="sd"> dnsDomain: cluster.local
&lt;/span>&lt;span class="sd"> serviceSubnet: 10.96.0.0/12
&lt;/span>&lt;span class="sd"> scheduler: {}&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w">&lt;/span>&lt;span class="nt">kind&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">ConfigMap&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w">&lt;/span>&lt;span class="nt">metadata&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">creationTimestamp&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s2">&amp;#34;2021-11-19T02:54:04Z&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">kubeadm-config&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">namespace&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">kube-system&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">resourceVersion&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s2">&amp;#34;210&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">uid&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">2567d366-2257-4114-8709-12b016cd1fe8&lt;/span>&lt;span class="w">
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>可以发现没有 &lt;code>podSubnet&lt;/code>，那就当是默认，按照 &lt;code>calico&lt;/code> 文档说明不用改 &lt;code>yaml&lt;/code>，正常应用。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">sudo kubectl --kubeconfig /etc/kubernetes/admin.conf apply -f calico.yaml
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>输出如下。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-plaintext" data-lang="plaintext">configmap/calico-config created
customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/caliconodestatuses.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipreservations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/kubecontrollersconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created
clusterrole.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrole.rbac.authorization.k8s.io/calico-node created
clusterrolebinding.rbac.authorization.k8s.io/calico-node created
service/calico-typha created
deployment.apps/calico-typha created
Warning: policy/v1beta1 PodDisruptionBudget is deprecated in v1.21+, unavailable in v1.25+; use policy/v1 PodDisruptionBudget
poddisruptionbudget.policy/calico-typha created
daemonset.apps/calico-node created
serviceaccount/calico-node created
deployment.apps/calico-kube-controllers created
serviceaccount/calico-kube-controllers created
poddisruptionbudget.policy/calico-kube-controllers created
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>出现了一个弃用警告，无视之，反正是 &lt;code>calico&lt;/code> 的问题。再检查下相关的 &lt;code>POD&lt;/code> 创建是否成功，用命令 &lt;code>kubectl get pod -n kube-system&lt;/code>，输出如下。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-plaintext" data-lang="plaintext">NAMESPACE NAME READY STATUS RESTARTS AGE
kube-system calico-kube-controllers-5d995d45d6-gwwrw 1/1 Running 0 5m29s
kube-system calico-node-sgb2x 0/1 Running 2 (49s ago) 5m29s
kube-system calico-typha-7df55cc78b-hpfkx 0/1 Pending 0 5m29s
kube-system coredns-7d89d9b6b8-c7sxl 1/1 Running 0 32m
kube-system coredns-7d89d9b6b8-tjsj8 1/1 Running 0 32m
kube-system etcd-vm 1/1 Running 0 32m
kube-system kube-apiserver-vm 1/1 Running 0 32m
kube-system kube-controller-manager-vm 1/1 Running 0 32m
kube-system kube-proxy-d64kh 1/1 Running 0 32m
kube-system kube-scheduler-vm 1/1 Running 0 32m
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>可以看到至少镜像是拉到了。&lt;/p>
&lt;p>&lt;code>calico-typha-7df55cc78b-hpfkx&lt;/code> 这个 POD 的 &lt;code>describe&lt;/code> 显示不能运行在 &lt;code>master&lt;/code> 节点。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-plaintext" data-lang="plaintext">Events:
Type Reason Age From Message
---- ------ ---- ---- -------
Warning FailedScheduling 56s (x9 over 8m57s) default-scheduler 0/1 nodes are available: 1 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn&amp;#39;t tolerate.
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>而 &lt;code>calico-node-sgb2x&lt;/code> 的日志显示需要 &lt;code>calico-typha&lt;/code> 才能运行。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-plaintext" data-lang="plaintext">2021-11-19 03:27:46.413 [ERROR][1674] confd/discovery.go 153: Didn&amp;#39;t find any ready Typha instances.
2021-11-19 03:27:46.413 [FATAL][1674] confd/startsyncerclient.go 48: Typha discovery enabled but discovery failed. error=Kubernetes service missing IP or port
bird: Unable to open configuration file /etc/calico/confd/config/bird6.cfg: No such file or directory
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>因为想要的是一个单节点集群，所以接下来把本节点的污点 &lt;code>node-role.kubernetes.io/master-&lt;/code> 给去掉。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">sudo kubectl --kubeconfig /etc/kubernetes/admin.conf taint nodes --all node-role.kubernetes.io/master-
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>输出&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-plaintext" data-lang="plaintext">node/vm untainted
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>再观察 &lt;code>kube-system&lt;/code> 里的 POD 状态。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-plaintext" data-lang="plaintext">NAME READY STATUS RESTARTS AGE
calico-kube-controllers-5d995d45d6-gwwrw 1/1 Running 0 11m
calico-node-sgb2x 1/1 Running 7 (38s ago) 11m
calico-typha-7df55cc78b-hpfkx 1/1 Running 0 11m
coredns-7d89d9b6b8-c7sxl 1/1 Running 0 38m
coredns-7d89d9b6b8-tjsj8 1/1 Running 0 38m
etcd-vm 1/1 Running 0 38m
kube-apiserver-vm 1/1 Running 0 38m
kube-controller-manager-vm 1/1 Running 0 38m
kube-proxy-d64kh 1/1 Running 0 38m
kube-scheduler-vm 1/1 Running 0 38m
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>可以看到所有的POD都已经进入&lt;code>Ready&lt;/code>状态。&lt;/p>
&lt;p>最后通过 &lt;code>kubectl get nodes -o wide&lt;/code> 检查节点状态。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-plaintext" data-lang="plaintext">NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME
vm Ready control-plane,master 39m v1.22.4 10.0.2.15 &amp;lt;none&amp;gt; Ubuntu 20.04.3 LTS 5.4.0-90-generic docker://20.10.11
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>到这里单节点集群就成功部署了。&lt;/p>
&lt;h2 id="总结">总结&lt;/h2>
&lt;p>之后还可以部署 dashboard 之类的应用验证，不想写了，浪费时间。&lt;/p>
&lt;p>写了一大堆又删掉了。&lt;/p>
&lt;p>如果一定要总结的话，k8s，学了进小厂吧，小厂不用；学了进大厂吧，大厂也不要你。&lt;/p></description></item><item><title>slackware 和虚拟机基本配置</title><link>https://nnnewb.github.io/blog/p/slackware-%E5%92%8C%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%9F%BA%E6%9C%AC%E9%85%8D%E7%BD%AE/</link><pubDate>Wed, 30 Dec 2020 11:11:56 +0800</pubDate><guid>https://nnnewb.github.io/blog/p/slackware-%E5%92%8C%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%9F%BA%E6%9C%AC%E9%85%8D%E7%BD%AE/</guid><description>&lt;p>slackware 是一个非常有极客味的 Linux 发行版，因为官方维护的包不多，基本靠 slackbuilds 续命。&lt;/p>
&lt;p>slackware 的一个特色是包管理系统不处理依赖关系，这一点劝退不少人。&lt;/p>
&lt;p>实际上，虽然我不是很赞同 &lt;a class="link" href="https://docs.slackware.com/start?id=slackware:package_and_dependency_management_shouldn_t_put_you_off_slackware" target="_blank" rel="noopener"
>这个观点&lt;/a> ，不过并不妨碍 slackware 成为可玩性相对高的 Linux 发行版之一（另外几个可玩性不错的发行版包括 Arch Linux 和 Gentoo）。&lt;/p>
&lt;p>这篇博文实际上就是安利下 slackware 并且简要介绍下怎么在虚拟机里搭建个基本环境来体验游玩。&lt;/p>
&lt;!-- more -->
&lt;h2 id="0x01-安装">0x01 安装&lt;/h2>
&lt;p>安装的参考文档太多了，个人认为主要的难点在分区和引导。毕竟不像其他更流行的发行版的 GUI 安装引导，对 fdisk 和 parted 这些工具不熟悉、对操作系统引导启动的一些基本概念、原理不了解的人很容易犯下错误而不自知。&lt;/p>
&lt;p>这里提供一篇之前在贴吧写的 &lt;a class="link" href="https://tieba.baidu.com/p/4863103375" target="_blank" rel="noopener"
>安装教程&lt;/a> ，不做赘述了。&lt;/p>
&lt;h2 id="0x02-桌面">0x02 桌面&lt;/h2>
&lt;p>对习惯了装完就有桌面的用户来说，安装完 slackware 之后遇到的第一个问题就是怎么进入桌面——甚至会问怎么登陆。&lt;/p>
&lt;p>这里就挂一张 gif 好了。&lt;/p>
&lt;p>&lt;img src="https://nnnewb.github.io/blog/blog/image/slackware-vm-setup/01.gif"
loading="lazy"
alt="01"
>&lt;/p>
&lt;p>假设没手贱在安装的时候把 x/kde/xfce 之类的软件包组给去掉的话，就不会有什么问题。&lt;/p>
&lt;p>如果需要自动进入桌面，需要手动修改 &lt;code>/etc/inittab&lt;/code> 文件，把默认的 runlevel 修改为 4 。&lt;/p>
&lt;p>具体怎么改，看 gif 。&lt;/p>
&lt;p>&lt;img src="https://nnnewb.github.io/blog/blog/image/slackware-vm-setup/02.gif"
loading="lazy"
alt="02"
>&lt;/p>
&lt;h2 id="0x03-slackpkg-包管理">0x03 slackpkg 包管理&lt;/h2>
&lt;p>如果用过 ubuntu ，那么下一个问题可能就是 &amp;ldquo;怎么没有 apt-get 命令？&amp;rdquo; 或者 &amp;ldquo;slackware 用什么命令安装软件？&amp;rdquo;&lt;/p>
&lt;p>答案是有好几个相关命令。&lt;/p>
&lt;ul>
&lt;li>installpkg&lt;/li>
&lt;li>removepkg&lt;/li>
&lt;li>upgradepkg&lt;/li>
&lt;li>makepkg&lt;/li>
&lt;li>explodepkg&lt;/li>
&lt;li>rpm2targz&lt;/li>
&lt;/ul>
&lt;p>大部分命令顾名思义，也不需要额外说明。如果说和 apt 或者 pacman 类似的一个统一的包管理器的话，那就是 slackpkg 。&lt;/p>
&lt;p>使用 slackpkg 之前，需要手动修改 /etc/slackpkg/mirrors 文件，选择一个网络状况比较好的软件源地址，把行开头的 # 号去掉。&lt;/p>
&lt;p>完事之后用命令 &lt;code>slackpkg update&lt;/code> 更新一下本地索引，就可以正常用了。&lt;/p>
&lt;p>常用的命令包括&lt;/p>
&lt;ul>
&lt;li>slackpkg search&lt;/li>
&lt;li>slackpkg file-search&lt;/li>
&lt;li>slackpkg install&lt;/li>
&lt;li>slackpkg install-new&lt;/li>
&lt;li>slackpkg upgrade&lt;/li>
&lt;li>slackpkg upgrade-all&lt;/li>
&lt;/ul>
&lt;p>具体不细说了，看参考链接，或者自己看看 &lt;code>man slackpkg&lt;/code> 或者 &lt;code>slackpkg help&lt;/code>&lt;/p>
&lt;p>此外还有个不常用的，和安装时的 &lt;code>setup&lt;/code> 风格比较类似的工具，&lt;code>pkgtool&lt;/code>。具体可以自己看看命令。&lt;/p>
&lt;h2 id="0x04-slackbuilds">0x04 SlackBuilds&lt;/h2>
&lt;p>用过 Arch Linux 的 AUR 的用户对这种第三方维护的软件包会比较熟悉， SlackBuilds 对这些用户来说就是另一个 AUR 而已。&lt;/p>
&lt;p>不同之处在于，SlackBuilds 需要手动下载脚本和源码，然后自己看 README 再运行编译。&lt;/p>
&lt;p>当然这不是说 SlackBuilds 没有类似 yaourt 或者 yay 之类的自动工具，你可以试试 sbopkg 。&lt;/p>
&lt;p>这里给个简单的例子，用 sbopkg 安装 fbterm 。&lt;/p>
&lt;p>&lt;img src="https://nnnewb.github.io/blog/blog/image/slackware-vm-setup/03.gif"
loading="lazy"
alt="03.gif"
>&lt;/p>
&lt;h2 id="0x05-编写-slackbuilds">0x05 编写 SlackBuilds&lt;/h2>
&lt;p>讲道理，slackware 常用的软件太少，基本全靠 slackbuilds 撑场面。如果 SlackBuilds 上也没有呢？&lt;/p>
&lt;p>那只能自己写吧。&lt;/p>
&lt;p>对于熟悉 bash 脚本的用户来说这不是什么难事。这篇 &lt;a class="link" href="https://slackwiki.com/Writing_A_SlackBuild_Script" target="_blank" rel="noopener"
>HOWTO 文章&lt;/a> 很好地说明了怎么写一个 SlackBuilds 脚本。&lt;/p>
&lt;h2 id="0x06-参与社区">0x06 参与社区&lt;/h2>
&lt;p>slackware 中文社区太小了，或者说根本不存在。&lt;/p>
&lt;p>能聊几句的基本只有贴吧（实际上现在也找不到人了）或者 GitHub 上（slackwarecn 社区也不活跃）。&lt;/p>
&lt;p>如果对 slackware 感兴趣，可以玩一玩，写几个常用软件的 SlackBuilds 脚本什么的。&lt;/p>
&lt;p>就这样吧。&lt;/p></description></item></channel></rss>