<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>分布式 on weakptr's 笔记</title><link>https://nnnewb.github.io/blog/tags/%E5%88%86%E5%B8%83%E5%BC%8F/</link><description>Recent content in 分布式 on weakptr's 笔记</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><lastBuildDate>Thu, 16 Dec 2021 15:00:00 +0800</lastBuildDate><atom:link href="https://nnnewb.github.io/blog/tags/%E5%88%86%E5%B8%83%E5%BC%8F/index.xml" rel="self" type="application/rss+xml"/><item><title>XA 事务从理论到实践</title><link>https://nnnewb.github.io/blog/p/xa-transaction-theory-to-practice/</link><pubDate>Thu, 16 Dec 2021 15:00:00 +0800</pubDate><guid>https://nnnewb.github.io/blog/p/xa-transaction-theory-to-practice/</guid><description>前言 有言道，纸上得来终觉浅，绝知此事要躬行。分布式事务的具体方案，看几篇文章就基本有了概念，但实际应用的机会很少。这不有点闲暇，就试试看把理论化作代码，在实践中检验。
#1 案例设计 采用分布式事务经典的转账案例：用户从银行A转账到银行B，银行A扣除余额，银行B增加余额。
XA事务官方规范文档给出的示意图如下。
事务模型
用 XA 事务描述，用户的转账操作发生在AP，AP调用TM注册全局事务后，调用银行A（RM）完成扣款（PREPARE），调用银行B（RM）完成增加余额（PREPARE），然后调用TM提交全局事务，TM回调银行A和B提交本地事务。
图示如下。
xa事务时序图
上面的时序图是读了 github.com/yedf/dtm 代码后胡乱分析出来的，图略去了错误处理的部分。根据这个时序图可以做出一个简单的服务划分设计。
案例服务划分
为了更好地观察服务的交互情况，引入了 Jaeger ，如果是为了简化整个案例代码考虑也可以不要。但大部分时候 Jaeger 应该是没什么存在感的。
nginx 反向代理将 AP 的接口还有 Bank1/Bank2的接口导出给用户访问，实际上案例中没有需要访问 Bank1/Bank2 接口的情况，所以 去掉 nginx 反向代理应该也没什么大关系。
#2 技术栈 所有服务使用docker-compose部署，kubernetes也没问题。
MySQL使用5.7版本，jaeger和nginx最新稳定版。AP/Bank服务都使用 Go 语言编写， 使用 Gin 作为 HTTP 服务框架，OpenTelemetry 跟踪，sqlx 做 ORM。
#3 接口设计 接口url设计有参考 Google APIs 规范，但并不是硬套 RESTful 。
AP服务提供接口
/v1alpha1/transfer 转账接口 Bank服务提供接口
/v1alpha1/trans_in 余额转入 /v1alpha1/trans_out 余额转出 /v1alpha1/tm_callback 事务回调，当AP提交事务或者回滚时，TM回调这个接口并告知需要提交还是回滚 TM服务提供接口</description></item><item><title>python 实现 redis 分布式锁</title><link>https://nnnewb.github.io/blog/p/python-%E5%AE%9E%E7%8E%B0-redis-%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81/</link><pubDate>Mon, 17 Dec 2018 14:57:00 +0800</pubDate><guid>https://nnnewb.github.io/blog/p/python-%E5%AE%9E%E7%8E%B0-redis-%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81/</guid><description>Intro 分布式不是啥黑魔法，究其理念无非是用多台服务器处理更多的请求。提高每秒处理的数据量，并发就不可避免了。
在单机并发的情况下，我们可以用 mutex，可以用 os 的文件锁，全局锁，多台服务器的并发就需要另一个持有并保护锁的角色了。
概述如何使用 redis 实现一个分布式锁。
为何是 Lua redis 保证了 lua 解释器执行脚本的事务性，即执行结果要么不可见，要么已完成。
参考这篇文档。
简单锁 简单锁指的是简单互斥锁，一旦锁定，则其他锁定请求都必须等待。
加锁 直觉的想法是通过 redis 的键来保持锁，故准备一个用于锁定互斥的名字（比如说 mutex-1）然后指定为键。
直接使用 set 是显然不正确的，如果临界区内程序崩溃或意外断网将导致死锁，所以 setnx 和 expire 是必选项。
加锁需要判断锁的键为空，才能加锁，这两步必须保证原子性，要么都执行，要么一个都不执行。幸好 redis 提供了这方面保证，只要使用 lua 脚本的话。
-- 加锁 if redis.call(&amp;#34;get&amp;#34;, KEYS[1]) == nil then if redis.call(&amp;#34;setnx&amp;#34;, KEYS[1], ARGV[1]) == 1 then redis.call(&amp;#34;expire&amp;#34;, KEYS[1], ARGV[2]) return 1 else return end end 上面的 lua 代码用 python 再封装一层，就是这样
def lock(key, expire): redis.eval( &amp;#39;&amp;#39;&amp;#39; -- 加锁 if redis.</description></item></channel></rss>