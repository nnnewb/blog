<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>kubernetes on weakptr's 笔记</title><link>https://nnnewb.github.io/blog/tags/kubernetes/</link><description>Recent content in kubernetes on weakptr's 笔记</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><lastBuildDate>Fri, 31 Dec 2021 18:30:00 +0800</lastBuildDate><atom:link href="https://nnnewb.github.io/blog/tags/kubernetes/index.xml" rel="self" type="application/rss+xml"/><item><title>记一次API响应时间优化</title><link>https://nnnewb.github.io/blog/p/an-api-response-time-optimize/</link><pubDate>Fri, 31 Dec 2021 18:30:00 +0800</pubDate><guid>https://nnnewb.github.io/blog/p/an-api-response-time-optimize/</guid><description>前言 刚接手管理后台的后端服务，先随便挑个什么东西下手看看。正好注意到一个简单的接口返回时间都蛮长的，于是拿刚从 opentelemetry 的 issue/pr 里抄来的 sqlmw 包装驱动来分析优化下性能。
0x01 性能分析 预判 下手前预估下可能存在瓶颈的地方。对于这次下手的接口（get_users），整个实现也没几行代码，只有两三个查询，数据量也不大，但是耗时有80ms+。
其他接口有快有慢，并没有表现出同时增加耗时，而且开发服务器架在内网，排除网络原因，大概还是服务本身的存在的问题。于是考虑瓶颈在数据库或代码中，但具体肯定是要看代码去分析的。既然判断是代码里的问题，那下一步就是测量下耗时情况了。
对于go，pprof虽然是个不错的主意，但实话说部署在 kubernetes 里，配 pprof 去拉结果有点麻烦，而且还有点点用不惯。正好这个项目里早就配置了 opentracing+jaeger做分布式跟踪，所以就直接抄一下 opentelemetry 的 otelsql ，把SQL查询的详细耗时情况记录下来，就可以开始分析了。
opentracing收集数据 otelsql 原理是用 sqlmw 在 sql 驱动层级上进行包装sql ==&amp;gt; sqlmw.Driver{mysql.Driver} 。go的sql调用sqlmw.Driver，sqlmw.Driver调用mysql.Driver，如此而已，具体不解释。
从otelsql借鉴下思路即可，现在 opentracing 已经和 opencensus 合并成了 opentelemetry，但项目也没法说升级就升级，毕竟项目架构设计稀烂，太多地方和 opentracing、jaeger-client 强耦合了。把otelsql里用sqlmw的部分抄出来，改成opentracing的方式创建span完事。
func (in *sqlInterceptor) ConnExecContext(ctx context.Context, conn driver.ExecerContext, query string, args []driver.NamedValue) (driver.Result, error) { span, ctx := opentracing.StartSpanFromContext(ctx, &amp;#34;ConnExecContext&amp;#34;) defer span.Finish() span.LogKV(&amp;#34;sql.query&amp;#34;, query) return conn.ExecContext(ctx, query, args) } 如此一来， 当go的sql库访问数据库的时候，就会在jaeger里记录一个span，可以清晰地看到耗时情况。</description></item><item><title>排查一个kubectl无反应的问题</title><link>https://nnnewb.github.io/blog/p/why-my-kubectl-not-responding/</link><pubDate>Mon, 27 Dec 2021 15:21:00 +0800</pubDate><guid>https://nnnewb.github.io/blog/p/why-my-kubectl-not-responding/</guid><description>懒得分段了，就当做是讲个故事吧。
背景大概是这样。
内网公共开发机上配置了 k3s 集群，同时后端开发工作也在这台开发机上进行（通过vscode remote-ssh）。因为公司太抠门，开发机只有117G硬盘容量，除去必要的开发工具、系统环境之类的东西，实际可用一直没超过50%，机器上又跑了很多东西，像是 gitlab-runner、docker的registry、MySQL、elasticsearch、开发集群服务等等，差不多每一两个星期都会出现 disk-pressure 的 taint，导致 pod 被 evicted。实话说能跑就很满足了，毕竟公司抠门到开发部门的上行带宽都贼小，如果把镜像推送到公网的registry去部署的话体验更差。
今天（周一）来公司之后调了下gitlab-ci，给一个前端项目做持续部署。因为前端对kubernetes这套不熟悉，也没有相关的服务器权限，总之就是很难让他们自己来。但是产品部门又喜欢提那种“按钮移到右上角”、“加个图片”之类的需求（对，我司还没有需求管理系统，开发就是个撸码的无情工具人），前端老是过来找我去部署下环境，就搞得摸鱼都摸不痛快。
所以，当当当~当~，整一个持续部署呗，反正是个纯前端项目，不用部署配套的后端代码，写个dockerfile再写个helm chart就差不多了，ci调了调构建镜像就完事，不过因为ci部署需要访问集群，所以又改了下.kube/config，删了之前尝试csr方式添加用户的时候加多的 user 和 context ，复制了一份挂载到 runner 容器里。
然后&amp;hellip;&amp;hellip;问题就来了。
同事忽然告诉我办公室的服务挂了，于是下意识地打出kgp，卡住。
等了一会儿，还是卡住。
又等了一会儿，坐不住了。试了下kubectl cluster-info，继续卡住。
开始慌了，想起今天的机器有点卡，先看看 free -h 有没有内存泄漏之类的问题导致阻塞，结果发现并没有，于是继续看 htop，cpu使用率也比较正常。再看df -h | grep -vE 'shm|overlay'，发现硬盘使用率96%（估计硬盘主控想死的心都有了，揪着4%的可用空间想把PE数平均到各个区块恐怕不容易）。
找到问题后松了口气，十有八九是又出现 evicted 了。二话不说直接 docker system df，看到30多G的 build cache 顿时惊了，肯定不是go的构建缓存（手动挂载优化了），那就是 node_modules 又立奇功了。node_modules=黑洞果然不是吹的。
清理完使用率恢复到63%，但依然有种不安感萦绕于心，于是再次尝试kgp，卡住。
等了一会儿，喝口水，继续卡着。
又等了一会儿，淦。
想了想，journalctl -r -u k3s看看日志，并没有什么发现，倒是注意到很多linkerd之类的我们部门经理搞事的时候遗留下来的玩意儿在报错，service mesh 我不熟，但寻思应该不会影响 kubectl 吧，k3s 本体的 api-server 应该不归 linkerd 管。更何况 linkerd 本身就没配好。再翻了翻看到下面的内容。
6 12月 25 21:16:07 office k3s[794]: I1225 13:16:07.</description></item><item><title>简单的ECK部署</title><link>https://nnnewb.github.io/blog/p/simple-eck-cluster-deployment/</link><pubDate>Tue, 30 Nov 2021 11:13:00 +0800</pubDate><guid>https://nnnewb.github.io/blog/p/simple-eck-cluster-deployment/</guid><description>前言 因为工作需要，得在自己搭建的集群里部署一个 Elasticsearch 。又因为是云端的集群，在 k8s 外用 docker 单独起一个 ES 明显更难维护（但部署更简单），于是选择用 ECK 。
ECK 就是 Elastic Cloud on Kubernetes 的缩写，可以理解成部署在 Kubernetes 上的 Elasticsearch 。当然不止 ES 。
部署 ES 的过程遇到几个问题记录下怎么解决的。
ES 使用自签名证书，导致 HTTP 不能连接。 ECK 需要安装 IK 分词插件。 ECK 默认密码每次部署都重新生成，而且默认用户权限过大。 ECK 默认没配 PVC ，数据没有持久化。 接下来逐个解决。
0x01 自签名证书 自签名证书解决方法有几个
改客户端，让客户端用自签名证书连接。很麻烦。 生成一个固定的证书，让ES和客户端都用这个证书，客户端和ES都要改。很麻烦。 禁用 ES 的自签名证书。 考虑到是私有的测试环境，不搞这些烦人的东西，直接禁用。
修改 YAML 如下。
apiVersion:elasticsearch.k8s.elastic.co/v1kind:Elasticsearchmetadata:name:elasticsearchspec:http:tls:selfSignedCertificate:disabled:true注意 spec.http.tls.selfSignedCertificate.disabled 这个字段。
参考文档：Orchestrating Elastic Stack applications - Access Elastic Stack services - TLS certificates</description></item><item><title>kubeadm安装实验集群记录</title><link>https://nnnewb.github.io/blog/p/kubernetes-manually-install-by-kubeadm/</link><pubDate>Thu, 25 Nov 2021 14:31:00 +0800</pubDate><guid>https://nnnewb.github.io/blog/p/kubernetes-manually-install-by-kubeadm/</guid><description>前言 好吧，如果仔细想想就会发现不管是 k3s 还是 ucloud 上的 k8s ，都没有一个是自己手动配置好的。虽说并不是至关重要的，但手动用 kubeadm 装一次 kubernetes 总不会有什么坏处。顺手做个笔记。参考资料列出如下。
https://kubernetes.io/zh/docs/setup/production-environment/tools/kubeadm/install-kubeadm/ https://mirrors.tuna.tsinghua.edu.cn/help/kubernetes/ https://computingforgeeks.com/deploy-kubernetes-cluster-on-ubuntu-with-kubeadm/ 系统配置 正式安装之前先确认一些系统级配置。
swapoff 简单的做法是 sudo swapoff -a 即可。之后改 fstab 把 swap 分区关掉。
iptables检查桥接流量 用 lsmod | grep bf_netfitler 检查有没有启用 bf_netfilter 模块，如果没有输出的话说明没加载，执行下面的命令。
cat &amp;lt;&amp;lt;EOF | sudo tee /etc/modules-load.d/k8s.conf br_netfilter EOF 会在 /etc/modules-load.d 下添加一个模块自动加载的配置。
cat &amp;lt;&amp;lt;EOF | sudo tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF 再在 /etc/sysctl.d/ 下添加一个配置，允许 iptables 查看桥接流量。
然后用 sysctl 重载配置。</description></item><item><title>k3s更新客户端证书的偷懒方法</title><link>https://nnnewb.github.io/blog/p/k3s-renew-client-ca-file-the-lazy-way/</link><pubDate>Mon, 11 Oct 2021 13:58:00 +0800</pubDate><guid>https://nnnewb.github.io/blog/p/k3s-renew-client-ca-file-the-lazy-way/</guid><description>前言 今天上内网服务器看了眼，准备调试下新代码，结果发现报错 You must logged in to the server (unauthorized) 。翻了半天的 KUBECONFIG 配置，发现啥也没错。换成 /etc/rancher/k3s/k3s.yaml 也不行。于是查了下 journalctl -r -u k3s ，发现日志 x509: certificate has expired or not yet valid: current time ... ，这就明确了是证书过期了。
于是又找了一圈如何给k3s更新证书，搜 how to renew client-ca-file 查出来的方法不是 kubeadm 就是改时间、换证书，总之&amp;hellip;麻烦，而且搜出来的文章可操作性都有点差，真要实践出真知也不能放公司的机器上，搞出点问题还得劝自己心平气和磨上一整天去解决。
于是终于找到个看起来能行的办法：重启。
操作 这个办法可操作性很强——反正情况不会变得更差了。因为办公室的服务器并不能保证24小时不断电，有时候白天上班机器是关机的，重启k3s无论如何不会导致问题变得更差——就算放着不管，过两天说不定也会断电重启下。
确认没人用服务之后直接上手。
sudo systemctl restart k3s 等待重启完成，测试下新的 k3s.yaml 能不能正常用。
export KUBECONFIG=/etc/rancher/k3s/k3s.yaml kubectl cluster-info Kubernetes control plane is running at https://192.168.2.175:6443 CoreDNS is running at https://192.168.2.175:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy Metrics-server is running at https://192.</description></item><item><title>csr 方式创建 kubernetes 用户出了点差错</title><link>https://nnnewb.github.io/blog/p/csr-%E6%96%B9%E5%BC%8F%E5%88%9B%E5%BB%BA-kubernetes-%E7%94%A8%E6%88%B7%E5%87%BA%E4%BA%86%E7%82%B9%E5%B7%AE%E9%94%99/</link><pubDate>Mon, 19 Jul 2021 09:52:38 +0800</pubDate><guid>https://nnnewb.github.io/blog/p/csr-%E6%96%B9%E5%BC%8F%E5%88%9B%E5%BB%BA-kubernetes-%E7%94%A8%E6%88%B7%E5%87%BA%E4%BA%86%E7%82%B9%E5%B7%AE%E9%94%99/</guid><description>越是在 kubernetes 的浑水里摸索，越是发现这就是个不顺手的锤子。
网上很多人喜欢把东西用不惯叫做懒，蠢，要是多反驳几句，那就还得搭上个“坏”的帽子。感觉吧，就这帮人看来，大神放个屁也值得学习，从里面“悟”出什么道理。
这帮人就跟传教士一样，但凡说个不字，就是在亵渎他们的“大神”。可谓人类迷惑行为。
好吧。技术别饭圈化行吗？
你说尤大强吗？Richard Stallman 是不是值得尊敬？Google 是不是最好的技术公司？Android 天下无敌？
然后全摆上神坛，挂上赛博天神的牌匾，插上网线一天 25 小时膜拜？
这帮人哪天搞个崇拜互联网和计算机的教派，把冯·诺依曼奉为先知我都不奇怪。
拜托，你们真的好怪欸。
完整脚本 #!/bin/bash -e # # 创建用户 gitlab 并授予权限 # # reference: # https://kubernetes.io/zh/docs/reference/access-authn-authz/certificate-signing-requests/#normal-user # if `gitlab` does not exists, # create csr and approve if ! kubectl get csr gitlab &amp;gt;/dev/null; then # create credential if [ ! -f gitlab.csr ]; then openssl genrsa -out gitlab.key 2048 openssl req -new -key gitlab.key -out gitlab.csr fi csr=$(cat gitlab.</description></item></channel></rss>